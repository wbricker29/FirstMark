# Phase Playbooks: Deep-Dive Execution Guidance

## Overview

This reference provides comprehensive per-phase execution guidance for the 5-phase multi-agent orchestration workflow. Each phase includes extended examples, edge cases, common failures, quality standards, tool usage recommendations, and decision points.

Use this reference when encountering unusual situations, complex scenarios, or when needing detailed guidance beyond the core SKILL.md workflow.

---

## Phase 1: Analyze (UNDERSTAND)

### Purpose

Establish comprehensive understanding of work requirements, gather necessary context, validate orchestration feasibility, and identify dependencies before planning agent deployment.

### Extended Examples

#### Example 1: Feature Implementation with Missing Context

**Scenario**: User requests "Add search filters to the product catalog" but provides no design specifications.

**Analysis steps:**
1. **Identify gaps in requirements:**
   - Which fields should be filterable?
   - What UI patterns exist for filters?
   - Are there performance constraints?
   - What's the expected user experience?

2. **Gather context proactively:**
   - Read existing product catalog code
   - Review UI component patterns
   - Check database schema for available fields
   - Look for similar filter implementations

3. **Ask clarifying questions:**
   ```
   Before proceeding, need clarity on:
   - Filter fields: [price, category, brand, rating]?
   - Filter UI: Sidebar, modal, or inline?
   - Performance: Expected product count? Pagination needed?
   - Existing patterns: Should match /users filter UI?
   ```

4. **Document understanding:**
   - Create analysis document with assumptions
   - List decided vs pending decisions
   - Map dependencies (UI ‚Üí API ‚Üí Database)

**Tool usage:**
- `Glob`: Find existing filter implementations (`**/*filter*.{ts,tsx}`)
- `Grep`: Search for filter patterns (`"Filter|filter" --type typescript`)
- `Read`: Examine product catalog files
- Output: Present clarifying questions to user before planning

**Decision point**: Do NOT proceed to planning until requirements are clear. Ambiguous requirements lead to wasted agent work.

#### Example 2: Refactoring Request with Broad Scope

**Scenario**: "Refactor the authentication system" - too vague, potentially massive scope.

**Analysis steps:**
1. **Explore current implementation:**
   ```
   - Use Grep to find all auth-related files
   - Map authentication flow (login ‚Üí session ‚Üí authorization)
   - Identify pain points (technical debt, security issues, maintainability)
   ```

2. **Scope boundaries:**
   ```
   Ask user:
   - What's driving this refactor? (Security, maintainability, performance?)
   - Which aspects: Session management? Auth middleware? Token handling? All?
   - Are there breaking changes acceptable?
   - Timeline constraints?
   ```

3. **Break down potential sub-tasks:**
   ```
   Possible scopes:
   A. Minimal: Refactor auth middleware only (narrow, safe)
   B. Moderate: Update session management + middleware (medium complexity)
   C. Comprehensive: Rebuild entire auth system (high risk, large scope)
   ```

4. **Risk assessment:**
   - Security implications (must maintain security guarantees)
   - Breaking changes (API compatibility)
   - Test coverage (existing tests as safety net)
   - Rollback strategy (if refactor fails)

**Tool usage:**
- `Grep`: Map auth usage (`"auth|Auth" --type typescript -C 2`)
- `Read`: Review critical auth files (middleware, session handler)
- `Bash`: Run tests to establish baseline (`pnpm test auth`)
- Output: Present scope options with risk/effort matrix

**Decision point**: Propose narrowest viable scope first. User can expand if needed. Prevents overwhelming orchestration.

#### Example 3: Bug Fix with System-Wide Impact

**Scenario**: "Fix the race condition in data sync" - potentially touches many components.

**Analysis steps:**
1. **Understand the bug:**
   - Reproduce the issue (if possible)
   - Identify affected components
   - Map data flow involved in race condition

2. **Assess blast radius:**
   ```
   Components involved:
   - Data sync service
   - State management
   - UI components reading sync state
   - Background workers
   - Database transactions
   ```

3. **Identify dependencies:**
   ```
   Fix requires:
   - Understanding async operation sequencing
   - Locking or coordination strategy
   - State consistency guarantees
   - Testing concurrent scenarios
   ```

4. **Validation strategy:**
   - How to verify fix works?
   - How to prevent regression?
   - What tests are needed?

**Tool usage:**
- `Grep`: Find sync-related code (`"sync|Sync" --type typescript`)
- `Read`: Study sync service implementation
- `Bash`: Check for existing race condition tests
- Output: Detailed analysis of race condition + proposed fix approach

**Decision point**: If analysis reveals complexity beyond 2-3 agents, propose incremental fix (address most critical path first, defer comprehensive solution).

### Edge Cases and Handling

#### Edge Case 1: Documentation Doesn't Exist

**Situation**: Orchestration requires project constitution/standards, but files don't exist.

**Handling:**
```
1. Ask user: "No project standards found. Options:
   a. Create constitution.md first (recommended)
   b. Proceed with general best practices
   c. User provides standards inline"

2. If user chooses (a):
   - Deploy /constitution command first
   - Resume orchestration after constitution created

3. If user chooses (b):
   - Document assumption: Using KISS/YAGNI/Quality principles
   - Proceed with orchestration
   - Note in deliverables: "Constitution recommended for future work"
```

**Key point**: Don't invent standards. Either use explicit standards or acknowledge using general principles.

#### Edge Case 2: Requirements Conflict

**Situation**: User requests "optimize for performance" AND "keep code simple" but these conflict for specific task.

**Handling:**
```
1. Identify specific conflict:
   "Performance optimization requires caching layer (adds complexity).
    This conflicts with 'keep simple' directive."

2. Present trade-off:
   "Options:
    a. Prioritize performance (add cache, accept complexity)
    b. Prioritize simplicity (skip cache, accept slower)
    c. Hybrid (simple cache, document trade-off)"

3. Wait for user decision before planning
```

**Key point**: Never resolve requirement conflicts autonomously. Always escalate to user.

#### Edge Case 3: Circular Dependencies Detected

**Situation**: Task A requires Task B, but Task B requires Task A (impossible to sequence).

**Handling:**
```
1. Document circular dependency:
   "Task A (implement UI) needs API shape from Task B
    Task B (implement API) needs UI requirements from Task A"

2. Identify break point:
   "Can break cycle by:
    a. Define API contract first (interface only)
    b. Implement stub API, then UI, then real API
    c. Design both together before implementing"

3. Propose resolution:
   "Recommended: Add Task 0 (design API contract)
    Then: Task A (UI) and Task B (API) can proceed"
```

**Tool usage:**
- Visual: Draw dependency graph to show cycle
- Document: Create interface definitions to break cycle

#### Edge Case 4: Scope Exceeds Context Budget

**Situation**: Pre-flight validation shows estimated 250k tokens (exceeds 200k limit).

**Handling:**
```
1. Run scripts/check_context_bounds.py to identify token usage

2. Identify reduction strategies:
   a. Split into multiple orchestrations (Phase 1 now, Phase 2 later)
   b. Reduce context files (only read essential files)
   c. Use smaller model (haiku instead of sonnet for simple agents)
   d. Parallelize less (reduces simultaneous context)

3. Present options:
   "Orchestration exceeds context budget by 50k tokens.
    Options:
    a. Split: Implement modules 1-3 now, 4-5 later (2 orchestrations)
    b. Optimize: Reduce reference docs loaded per agent
    c. Sequential: Deploy agents one-by-one (slower but lower peak)"

4. Adjust plan based on user choice
```

**Tool usage:**
- `Bash`: Run `scripts/check_context_bounds.py --phase 1 --files <list>`
- Output: Token usage report with breakdown

### Common Failures and Prevention

#### Failure 1: Proceeding with Ambiguous Requirements

**Symptom**: Agent produces work that doesn't match user intent.

**Root cause**: Skipped clarifying questions during analysis.

**Prevention:**
```
Checklist before planning:
  ‚úÖ Deliverables are clearly defined (what files, what changes)
  ‚úÖ Quality criteria are explicit (what makes it "done"?)
  ‚úÖ Constraints are documented (what NOT to change)
  ‚úÖ Edge cases are identified (what unusual scenarios to handle)
  ‚ùå If any unchecked, ask clarifying questions
```

**Recovery**: Pause orchestration, clarify with user, restart planning phase.

#### Failure 2: Missing Critical Context

**Symptom**: Agent fails because it needs information not available in briefing.

**Root cause**: Incomplete context gathering during analysis.

**Prevention:**
```
Context gathering checklist:
  ‚úÖ Project structure understood (where things live)
  ‚úÖ Standards/conventions identified (constitution, style guide)
  ‚úÖ Dependencies mapped (what depends on what)
  ‚úÖ Existing patterns found (how similar things are done)
  ‚úÖ Integration points known (APIs, interfaces, contracts)
```

**Recovery**: Provide missing context to agent, allow retry with adjusted briefing.

#### Failure 3: Underestimating Task Complexity

**Symptom**: Single agent takes hours or fails due to scope too large.

**Root cause**: Didn't break down task into sub-tasks during analysis.

**Prevention:**
```
Complexity assessment:
  - Single agent task should be completable in ~15-30 minutes
  - If task involves 3+ distinct phases, split into multiple agents
  - If task touches 5+ files, consider parallelizing
  - If task requires extensive design, add planning agent first
```

**Recovery**: Pause agent, break task into smaller agents, redeploy with focused scopes.

### Quality Standards for Analysis Phase

**Phase complete when:**

- ‚úÖ Task scope clearly defined in 2-3 sentences
- ‚úÖ All deliverables specified (files, changes, artifacts)
- ‚úÖ Dependencies mapped (what needs what)
- ‚úÖ Context files identified (what to read)
- ‚úÖ Validation criteria defined (how to verify success)
- ‚úÖ Edge cases documented (unusual scenarios to handle)
- ‚úÖ Ambiguities resolved (no unanswered questions)
- ‚úÖ Context budget validated (won't exceed token limits)

**Quality check questions:**
```
1. Can I describe each sub-task in one sentence? (If no: scope unclear)
2. Do I know which files each agent will modify? (If no: incomplete analysis)
3. Can agents run independently or do they need sequencing? (Determines coordination)
4. What would "success" look like for each agent? (Defines validation)
5. Are there any blockers or missing information? (Identifies risks)
```

### Tool Usage Recommendations

**Primary tools for Phase 1:**

| Tool | Use Case | Example |
|------|----------|---------|
| `Glob` | Find files by pattern | `Glob("**/*auth*.ts")` |
| `Grep` | Search code for patterns | `Grep("import.*Auth", type="typescript")` |
| `Read` | Examine specific files | `Read("lib/auth.ts")` |
| `Bash` | Run validation scripts | `Bash("pnpm test")` to establish baseline |
| `Task` | Deploy exploration agent | `Task(subagent_type="Explore", prompt="Map auth flows")` |

**Tool selection criteria:**
- Use `Glob` when searching by filename pattern
- Use `Grep` when searching by code content
- Use `Read` when examining specific known files
- Use `Task` (Explore agent) when analysis is complex/exploratory
- Use `Bash` to validate current state (tests pass, builds work)

### Decision Points

#### Decision 1: Explore Agent vs Direct Analysis

**When to deploy Explore agent:**
- Codebase unfamiliar
- Task touches many unknown files
- Need to map architectural patterns
- Analysis itself is complex (>30 min)

**When to analyze directly:**
- Small, focused scope
- Familiar codebase
- Clear requirements
- Simple dependency structure

**Decision criteria**: If you would spend >10 file reads exploring, deploy Explore agent instead.

#### Decision 2: Ask User vs Make Assumption

**Ask user when:**
- Design decisions (UI patterns, API structure)
- Requirement ambiguities (what features to include)
- Trade-off choices (performance vs simplicity)
- Breaking changes (acceptable or not?)
- Priority conflicts (what's more important?)

**Make assumption when:**
- Following established patterns (use existing conventions)
- Technical implementation details (how to structure code)
- File organization (where to put new files)
- Test structure (how to organize tests)

**Rule**: Business/product decisions ‚Üí ask user. Technical implementation ‚Üí use judgment.

#### Decision 3: Validate Feasibility Before Planning

**Always validate:**
- Context budget (run check_context_bounds.py)
- Dependency cycles (check for circular dependencies)
- Agent availability (do necessary agent types exist?)
- Resource availability (files, APIs, data exist?)

**Proceed only if:**
- ‚úÖ Context budget sufficient
- ‚úÖ Dependencies are acyclic
- ‚úÖ Agent types available
- ‚úÖ Resources accessible

**If validation fails**: Adjust scope or approach before planning.

---

## Phase 2: Plan (DESIGN SUB-AGENT STRATEGY)

### Purpose

Design optimal sub-agent deployment strategy, establish coordination protocols, define agent scopes and interfaces, and secure user approval before execution.

### Extended Examples

#### Example 1: Simple Parallel Decomposition

**Scenario**: Add validation functions for 4 different data types (independent tasks).

**Planning steps:**
1. **Identify sub-tasks:**
   ```
   Task 1: Email validation function
   Task 2: Phone number validation function
   Task 3: Credit card validation function
   Task 4: Address validation function
   ```

2. **Validate independence:**
   ```
   ‚úÖ Each creates separate file (lib/validators/email.ts, etc.)
   ‚úÖ No shared types (each self-contained)
   ‚úÖ No dependencies between validators
   ‚úÖ Can test independently
   ‚Üí Parallel execution safe
   ```

3. **Define agent scopes:**
   ```
   Agent A (task-implementor):
     Scope: Implement email validation with regex patterns
     Files: lib/validators/email.ts, lib/validators/email.test.ts
     Context: Existing validator pattern from lib/validators/string.ts
     Validation: 80%+ coverage, all edge cases tested

   Agent B (task-implementor):
     Scope: Implement phone validation with international formats
     Files: lib/validators/phone.ts, lib/validators/phone.test.ts
     Context: Existing validator pattern
     Validation: 80%+ coverage, support US/EU formats

   [Similar for Agents C and D]
   ```

4. **Establish coordination:**
   ```
   Execution: Parallel (all 4 agents deploy simultaneously)
   Handoff: None required (independent)
   Merge: Auto-merge (no conflicts possible)
   Validation: Run all tests after completion
   ```

**Plan presentation:**
```
ü§ñ Sub-Agent Orchestration Plan

Agents to deploy: 4
‚îú‚îÄ Agent A (task-implementor): Email validation
‚îú‚îÄ Agent B (task-implementor): Phone validation
‚îú‚îÄ Agent C (task-implementor): Credit card validation
‚îî‚îÄ Agent D (task-implementor): Address validation

Execution strategy: Parallel (all agents simultaneously)
Coordination points: None (fully independent)
Expected deliverables: 8 files (4 implementations + 4 test suites)
Validation gates: format|lint|types|tests|coverage(80%+)

Estimated time: ~20 minutes
Context budget: 45k tokens (well within limits)

Proceed? (y/n)
```

#### Example 2: Sequential Pipeline with Handoffs

**Scenario**: Optimize database queries - requires analysis, implementation, validation, documentation.

**Planning steps:**
1. **Identify phases:**
   ```
   Phase A: Analysis (identify slow queries)
   Phase B: Implementation (optimize queries)
   Phase C: Validation (verify performance)
   Phase D: Documentation (update perf docs)
   ```

2. **Map dependencies:**
   ```
   B depends on A (needs list of slow queries)
   C depends on B (needs optimized code)
   D depends on C (needs performance results)
   ‚Üí Sequential execution required
   ```

3. **Define agent scopes:**
   ```
   Agent A (Explore):
     Scope: Profile database queries, identify top 5 slowest
     Output: query-analysis.md with query locations + metrics
     Time: ~10 minutes

   Agent B (task-implementor):
     Scope: Optimize top 3 queries from analysis
     Input: query-analysis.md
     Files: Modify query implementations
     Time: ~25 minutes

   Agent C (systematic-debugger):
     Scope: Run performance benchmarks, verify improvements
     Input: Modified query files
     Output: benchmark-results.md
     Time: ~15 minutes

   Agent D (documentation-manager):
     Scope: Update performance documentation
     Input: benchmark-results.md
     Files: docs/performance.md
     Time: ~10 minutes
   ```

4. **Design handoff protocol:**
   ```
   A ‚Üí B: Pass query-analysis.md (top 3 queries + file locations)
   B ‚Üí C: Pass modified files + optimization summary
   C ‚Üí D: Pass benchmark results (before/after metrics)
   ```

**Plan presentation:**
```
ü§ñ Sub-Agent Orchestration Plan

Agents to deploy: 4
‚îú‚îÄ Agent A (Explore): Profile queries ‚Üí Identify slow queries
‚îú‚îÄ Agent B (task-implementor): Optimize top 3 queries
‚îú‚îÄ Agent C (systematic-debugger): Benchmark performance
‚îî‚îÄ Agent D (documentation-manager): Update docs

Execution strategy: Sequential (4 phases)
Coordination points:
  - A ‚Üí B: Query analysis report
  - B ‚Üí C: Optimized code + summary
  - C ‚Üí D: Benchmark results

Expected deliverables:
  - query-analysis.md
  - Modified query files (3-5 files)
  - benchmark-results.md
  - Updated docs/performance.md

Validation gates: format|lint|types|tests|performance(>2x improvement)

Estimated time: ~60 minutes
Context budget: 65k tokens

Proceed? (y/n)
```

#### Example 3: Hybrid Strategy (Analysis ‚Üí Parallel ‚Üí Synthesis)

**Scenario**: Add i18n support across UI, API, and email templates.

**Planning steps:**
1. **Identify workflow structure:**
   ```
   Wave 1 (Sequential): Setup i18n infrastructure
   Wave 2 (Parallel): Apply i18n to different areas
   Wave 3 (Sequential): Validate consistency
   ```

2. **Design agent deployment:**
   ```
   Wave 1:
     Agent A: Create i18n infrastructure (lib/i18n.ts, locale files)
     ‚Üí Foundation for other agents

   Wave 2 (parallel after Wave 1):
     Agent B: Add i18n to UI components
     Agent C: Add i18n to API messages
     Agent D: Add i18n to email templates
     ‚Üí Independent implementations

   Wave 3:
     Agent E: Validate i18n consistency
     Agent F: Update documentation
   ```

3. **Define coordination:**
   ```
   A completes ‚Üí Handoff i18n API to B, C, D
   B, C, D complete ‚Üí Wait for all, then deploy E
   E completes ‚Üí Deploy F
   ```

**Plan presentation:**
```
ü§ñ Sub-Agent Orchestration Plan

Agents to deploy: 6

Wave 1 (Foundation):
‚îú‚îÄ Agent A (task-implementor): i18n infrastructure

Wave 2 (Parallel implementation):
‚îú‚îÄ Agent B (task-implementor): i18n for UI
‚îú‚îÄ Agent C (task-implementor): i18n for API
‚îî‚îÄ Agent D (task-implementor): i18n for emails

Wave 3 (Validation):
‚îú‚îÄ Agent E (principle-evaluator): Consistency check
‚îî‚îÄ Agent F (documentation-manager): Update docs

Execution strategy: Hybrid (sequential ‚Üí parallel ‚Üí sequential)
Coordination points:
  - Wave 1 ‚Üí 2: i18n API + usage patterns
  - Wave 2 ‚Üí 3: All i18n implementations complete

Expected deliverables:
  - lib/i18n.ts + locale files
  - ~20 modified component files
  - ~10 modified API files
  - ~5 modified email templates
  - docs/i18n-guide.md

Validation gates: format|lint|types|tests|coverage(80%+)

Estimated time: ~90 minutes (30 + 45 + 15)
Context budget: 110k tokens

Proceed? (y/n)
```

### Edge Cases and Handling

#### Edge Case 1: Agent Type Doesn't Exist

**Situation**: Need specialized agent (e.g., "SQL optimizer") but no such agent type exists.

**Handling:**
```
Option A: Use general-purpose agent with detailed instructions
  Agent: task-implementor
  Brief: "Act as SQL optimization expert. Analyze queries for N+1
         problems, missing indexes, inefficient joins..."

Option B: Deploy Explore agent first for analysis, then task-implementor
  Agent 1 (Explore): "Profile and identify optimization opportunities"
  Agent 2 (task-implementor): "Implement optimizations from analysis"

Option C: Ask user if specialized agent should be created
  "This task would benefit from specialized 'SQL optimizer' agent.
   For now, will use task-implementor with detailed SQL expertise brief.
   Consider creating specialized agent for future?"

Choose: Usually Option A (general-purpose with instructions)
```

#### Edge Case 2: User Requests Infeasible Parallelization

**Situation**: User says "deploy all agents in parallel" but tasks have dependencies.

**Handling:**
```
1. Explain dependency conflict:
   "Agent B (API implementation) requires types from Agent A (schema).
    Cannot deploy in parallel due to dependency."

2. Propose alternative:
   "Can partially parallelize:
    Wave 1: Agent A (schema)
    Wave 2: Agents B, C, D in parallel (all depend on A, independent of each other)"

3. Show trade-off:
   "Fully sequential: 80 minutes
    Hybrid approach: 50 minutes (saves 30 min)
    Risk: None (respects dependencies)"

4. Wait for user approval of adjusted plan
```

#### Edge Case 3: Context Budget Tight

**Situation**: Plan exceeds context budget during planning phase.

**Handling:**
```
1. Run scripts/check_context_bounds.py on planned agents

2. Identify context-heavy agents:
   "Agent B will load 15 files (40k tokens)
    Agent D will load 20 files (50k tokens)
    Total: 125k / 200k tokens"

3. Reduce context:
   Strategy A: Load only essential files
   Strategy B: Use file excerpts (line ranges) instead of full files
   Strategy C: Split agent into smaller scopes

4. Adjust plan:
   "Modified Agent B brief: Read only lib/core.ts (lines 1-200)
    instead of all 15 files. Reduces context by 25k tokens."

5. Re-validate budget
```

#### Edge Case 4: No Clear Validation Criteria

**Situation**: User request doesn't specify how to validate success.

**Handling:**
```
1. Propose validation based on task type:
   - Code implementation: Tests pass + coverage ‚â•80%
   - Refactoring: Behavior unchanged (all tests pass)
   - Documentation: Completeness + accuracy + clarity
   - Bug fix: Reproduction test passes + no regression

2. Present to user:
   "Validation plan:
    ‚úÖ All existing tests pass (no regression)
    ‚úÖ New tests for added functionality
    ‚úÖ Coverage ‚â•80% on modified files
    ‚úÖ Linting and formatting pass
    ‚úÖ Type checking passes

    Additional validation needed? (y/n)"

3. Adjust based on user feedback
```

### Common Failures and Prevention

#### Failure 1: Over-Ambitious Parallelization

**Symptom**: Parallel agents conflict, modify same files, produce incompatible outputs.

**Root cause**: Incorrectly assumed task independence, skipped independence validation.

**Prevention:**
```
Independence validation checklist (perform during planning):
  For each pair of parallel agents:
    ‚úÖ Modify different files OR different sections
    ‚úÖ Create different types/interfaces (no naming conflicts)
    ‚úÖ No information flow required between agents
    ‚úÖ Can validate independently
    ‚úÖ Merge strategy is clear

  If ANY check fails ‚Üí Agents are NOT independent ‚Üí Use sequential
```

**Recovery**: Switch to sequential or hybrid strategy, redeploy agents one-by-one.

#### Failure 2: Vague Agent Scopes

**Symptom**: Agent produces output that doesn't match expectations or exceeds scope.

**Root cause**: Agent brief was too vague, lacked specific constraints.

**Prevention:**
```
Scope definition checklist:
  ‚úÖ Specific task described in 1-2 sentences
  ‚úÖ Files to modify are listed explicitly
  ‚úÖ Constraints are documented (what NOT to do)
  ‚úÖ Expected output format specified
  ‚úÖ Integration points defined (how output will be used)
  ‚úÖ Time estimate provided (helps scope sizing)

Bad scope: "Improve the search feature"
Good scope: "Add price range filter to product search in components/ProductSearch.tsx.
            Use existing FilterControls pattern. Update search API query parameters.
            Do not modify search algorithm or ranking logic."
```

#### Failure 3: Missing Handoff Information

**Symptom**: Sequential agent fails because it doesn't have information from previous agent.

**Root cause**: Handoff protocol not defined during planning.

**Prevention:**
```
Handoff definition checklist:
  For each sequential transition (Agent A ‚Üí Agent B):
    ‚úÖ What information B needs from A (specific)
    ‚úÖ Format of handoff (file path, summary, artifact)
    ‚úÖ What B should NOT get (avoid context bloat)
    ‚úÖ Where handoff information lives (file, in-memory)

Example handoff plan:
  "Agent A ‚Üí Agent B:
   Pass: File paths of modified components (list)
   Pass: Summary of changes (3-5 bullets)
   Pass: Validation results (pass/fail + coverage %)
   Do NOT pass: Full implementation details (B will read files)
   Do NOT pass: Design discussions (not needed for B's task)"
```

### Quality Standards for Planning Phase

**Phase complete when:**

- ‚úÖ All sub-agents identified with clear types
- ‚úÖ Each agent has specific, bounded scope
- ‚úÖ Execution strategy defined (parallel/sequential/hybrid)
- ‚úÖ Dependencies mapped and validated (no cycles)
- ‚úÖ Handoff protocols defined (for sequential agents)
- ‚úÖ Context budget validated (within limits)
- ‚úÖ Validation gates specified per agent
- ‚úÖ User approval obtained (plan presented and confirmed)

**Quality check questions:**
```
1. Can each agent scope be described in 1-2 sentences? (If no: scope too vague)
2. Is execution order unambiguous? (If no: dependencies unclear)
3. Are parallel agents truly independent? (If no: will conflict)
4. Are handoff points defined? (If no: sequential agents may fail)
5. Is context budget realistic? (If no: will exceed limits)
6. Would I approve this plan if I were the user? (If no: revise)
```

### Tool Usage Recommendations

**Primary tools for Phase 2:**

| Tool | Use Case | Example |
|------|----------|---------|
| None | Planning is primarily cognitive | Design agent strategy mentally |
| `Read` | Review templates for plan | `Read("assets/orchestration-plan.tmpl")` |
| `Bash` | Validate scripts exist | `Bash("ls scripts/validate_orchestration.py")` |

**Note**: Planning phase is mostly cognitive work. Primary output is structured plan presented to user.

### Decision Points

#### Decision 1: Parallel vs Sequential

**Choose parallel when:**
- ‚úÖ Tasks are independent (validated)
- ‚úÖ No information flow between agents
- ‚úÖ Speed is important
- ‚úÖ Context budget allows multiple agents

**Choose sequential when:**
- ‚úÖ Dependencies exist
- ‚úÖ Information must flow between agents
- ‚úÖ Later agents need outputs from earlier agents
- ‚úÖ Context budget is tight

**Validation**: Run independence checklist for parallel candidates.

#### Decision 2: How Many Agents?

**Too few agents** (under-decomposition):
- Single agent takes >45 minutes
- Agent scope spans multiple domains
- High risk of failure due to complexity

**Too many agents** (over-decomposition):
- Each agent takes <5 minutes
- Coordination overhead > execution time
- Context budget wasted on handoffs

**Right-sizing**:
- Target: 15-30 minutes per agent
- Max: 3-5 agents per orchestration (sweet spot)
- Consider: Can 2 agents be combined? Should 1 agent be split?

#### Decision 3: When to Present Plan to User

**Always present plan when:**
- Complex orchestration (3+ agents)
- User provided high-level request (need confirmation on approach)
- Multiple valid strategies exist (user should choose)
- Significant time investment (>30 min total)

**Can skip presentation when:**
- User provided detailed, explicit plan
- Trivial orchestration (2 simple agents)
- User explicitly requested auto-execution

**Default**: Present plan. Better to over-communicate than under-communicate.

---

## Phase 3: Execute (DEPLOY & COORDINATE)

### Purpose

Deploy sub-agents with clear briefings, manage coordination and handoffs, monitor execution, track progress, and handle issues as they arise.

### Extended Examples

#### Example 1: Parallel Deployment (4 Independent Agents)

**Scenario**: Implement 4 validation utilities (from planning example).

**Execution steps:**

1. **Prepare all agent briefings:**
   ```
   [Prepare 4 detailed briefings offline before deploying]

   Agent A brief: Email validation
   Agent B brief: Phone validation
   Agent C brief: Credit card validation
   Agent D brief: Address validation
   ```

2. **Deploy all agents simultaneously (single message with 4 Task calls):**
   ```
   Deploy all 4 task-implementor agents in parallel:
   - Agent A: Email validation
   - Agent B: Phone validation
   - Agent C: Credit card validation
   - Agent D: Address validation
   ```

3. **Wait for all agents to complete:**
   ```
   Monitor progress:
   ‚úÖ Agent A: Completed in 18 minutes
   ‚úÖ Agent B: Completed in 22 minutes
   ‚úÖ Agent C: Completed in 20 minutes
   ‚úÖ Agent D: Completed in 25 minutes

   Total elapsed: 25 minutes (longest agent)
   ```

4. **Collect outputs:**
   ```
   Agent A delivered: lib/validators/email.ts, lib/validators/email.test.ts
   Agent B delivered: lib/validators/phone.ts, lib/validators/phone.test.ts
   Agent C delivered: lib/validators/card.ts, lib/validators/card.test.ts
   Agent D delivered: lib/validators/address.ts, lib/validators/address.test.ts

   Total: 8 files created
   ```

**Coordination notes:**
- No handoffs needed (parallel execution)
- No conflicts (different files)
- Proceed directly to validation phase

#### Example 2: Sequential Deployment (4-Agent Pipeline)

**Scenario**: Query optimization pipeline (from planning example).

**Execution steps:**

1. **Deploy Agent A (Explore):**
   ```
   Deploy: Agent A (Explore)
   Scope: Profile database queries, identify slow queries
   Context: Database query files in lib/queries/
   Output: query-analysis.md

   [Wait for completion - 12 minutes]

   ‚úÖ Agent A completed:
      Created: query-analysis.md
      Identified: 5 slow queries (2 in lib/users.ts, 3 in lib/posts.ts)
   ```

2. **Extract handoff information for Agent B:**
   ```
   Read query-analysis.md (lines showing top 3 queries)

   Handoff info:
   - Query 1: lib/users.ts (line 45) - N+1 problem, 850ms avg
   - Query 2: lib/posts.ts (line 120) - Missing index, 650ms avg
   - Query 3: lib/posts.ts (line 200) - Inefficient join, 550ms avg
   ```

3. **Deploy Agent B (task-implementor):**
   ```
   Deploy: Agent B (task-implementor)
   Scope: Optimize top 3 slow queries
   Context:
     - query-analysis.md (top 3 queries section)
     - lib/users.ts (lines 40-60, focus on query at line 45)
     - lib/posts.ts (lines 115-125, 195-205)
   Instructions:
     - Fix N+1 in users query (use JOIN instead of multiple queries)
     - Add database index for posts query 1
     - Optimize join in posts query 2

   [Wait for completion - 28 minutes]

   ‚úÖ Agent B completed:
      Modified: lib/users.ts, lib/posts.ts, migrations/add-index.sql
      Optimizations: N+1 resolved, index added, join optimized
   ```

4. **Extract handoff information for Agent C:**
   ```
   Handoff info:
   - Modified files: lib/users.ts, lib/posts.ts
   - Optimization summary: N+1 fix, new index, optimized join
   - Expected improvement: 850ms ‚Üí ~100ms (users), 650ms ‚Üí ~50ms (posts 1), 550ms ‚Üí ~200ms (posts 2)
   ```

5. **Deploy Agent C (systematic-debugger):**
   ```
   Deploy: Agent C (systematic-debugger)
   Scope: Run performance benchmarks, verify improvements
   Context:
     - lib/users.ts (modified query)
     - lib/posts.ts (modified queries)
     - tests/performance/ (benchmark utilities)
   Instructions:
     - Benchmark modified queries (10 runs each)
     - Compare before/after metrics
     - Verify no functionality regressions

   [Wait for completion - 18 minutes]

   ‚úÖ Agent C completed:
      Created: benchmark-results.md
      Results: Users 850‚Üí95ms (11x), Posts 650‚Üí48ms (13x), Posts 550‚Üí215ms (2.5x)
      Validation: All tests pass, no regressions
   ```

6. **Deploy Agent D (documentation-manager):**
   ```
   Deploy: Agent D (documentation-manager)
   Scope: Update performance documentation
   Context:
     - benchmark-results.md
     - docs/performance.md
   Instructions:
     - Document optimizations made
     - Add before/after metrics
     - Update performance expectations

   [Wait for completion - 10 minutes]

   ‚úÖ Agent D completed:
      Modified: docs/performance.md
      Added: Query optimization case study section
   ```

**Total elapsed time:** 68 minutes (sequential pipeline)

#### Example 3: Hybrid Deployment (3 Waves)

**Scenario**: i18n implementation (from planning example).

**Execution steps:**

**Wave 1 (Sequential - Foundation):**
```
Deploy: Agent A (task-implementor)
Scope: Create i18n infrastructure
Deliverables: lib/i18n.ts, locales/en.json, locales/es.json

[Wait for completion - 25 minutes]

‚úÖ Agent A completed:
   Created: lib/i18n.ts (i18n hook + provider)
   Created: locales/en.json, locales/es.json (structure)
   Updated: app/layout.tsx (added i18n provider)
```

**Wave 2 (Parallel - Implementation):**
```
Extract handoff info from Agent A:
  - i18n API: useTranslation() hook
  - Translation key format: "section.subsection.key"
  - Locale file structure: Nested JSON objects

Deploy 3 agents in parallel:

Agent B (task-implementor):
  Scope: Add i18n to UI components
  Context: lib/i18n.ts (API), components/**/*.tsx
  Pattern: Replace hardcoded strings with t('key')

Agent C (task-implementor):
  Scope: Add i18n to API messages
  Context: lib/i18n.ts (API), app/api/**/*.ts
  Pattern: Use i18n in server-side responses

Agent D (task-implementor):
  Scope: Add i18n to email templates
  Context: lib/i18n.ts (API), lib/email/**/*.ts
  Pattern: Use i18n in email content generation

[Deploy all 3 simultaneously]

[Wait for all to complete]

‚úÖ Agent B completed in 30 minutes:
   Modified: 15 component files
   Updated: locales/en.json, locales/es.json (UI strings)

‚úÖ Agent C completed in 25 minutes:
   Modified: 8 API route files
   Updated: locales/en.json, locales/es.json (API messages)

‚úÖ Agent D completed in 20 minutes:
   Modified: 5 email template files
   Updated: locales/en.json, locales/es.json (email strings)

Total Wave 2 time: 30 minutes (longest agent)
```

**Wave 3 (Sequential - Validation):**
```
Deploy: Agent E (principle-evaluator)
Scope: Validate i18n consistency
Context: All modified files from B, C, D
Checks: All strings externalized, consistent key naming, no hardcoded text

[Wait for completion - 12 minutes]

‚úÖ Agent E completed:
   Report: 98% strings externalized (3 exceptions in error handlers - documented)
   Validation: Consistent key naming, proper i18n usage
   Issues: None critical

Deploy: Agent F (documentation-manager)
Scope: Update documentation
Context: Agent E report, lib/i18n.ts
Deliverables: docs/i18n-guide.md

[Wait for completion - 10 minutes]

‚úÖ Agent F completed:
   Created: docs/i18n-guide.md (usage guide)
   Updated: README.md (added i18n section)
```

**Total orchestration time:** 77 minutes (25 + 30 + 22)

### Edge Cases and Handling

#### Edge Case 1: Agent Exceeds Time Estimate

**Situation**: Agent expected to complete in 20 minutes, still running after 40 minutes.

**Handling:**
```
1. Check agent status:
   - Is it making progress? (new outputs appearing)
   - Is it stuck? (no activity for 10+ minutes)
   - Is it in loop? (repeating same actions)

2. If making progress:
   "Agent B is still working (40 min elapsed, expected 20 min).
    Task scope may have been larger than estimated.
    Continue waiting? (y/n)"

3. If stuck or looping:
   "Agent B appears stuck (no progress for 15 minutes).
    Options:
    a. Interrupt and redeploy with adjusted scope
    b. Provide additional context/guidance
    c. Wait longer (may resolve itself)"

4. Learn from experience:
   Document: "Task-implementor on refactoring tasks takes 2x estimate"
   Adjust future estimates accordingly
```

#### Edge Case 2: Context Overflow During Execution

**Situation**: Agent runs out of context mid-execution.

**Handling:**
```
1. Immediate action:
   "Agent C encountered context overflow.
    Current token usage: 185k / 200k"

2. Reduce context:
   Options:
   a. Remove unnecessary files from agent context
   b. Use file excerpts instead of full files
   c. Split agent scope into 2 smaller agents

3. Redeploy with reduced context:
   Agent C1: Focus on core implementation (reduced scope)
   Agent C2: Handle remaining work (if needed)

4. Prevention for remaining agents:
   Check context budget before deploying next agent
   Use scripts/check_context_bounds.py
```

#### Edge Case 3: Agent Produces Unexpected Output

**Situation**: Agent completes but output doesn't match expectations.

**Handling:**
```
1. Analyze mismatch:
   Expected: Modified 3 files with caching logic
   Actual: Modified 5 files, refactored entire module

2. Determine cause:
   - Brief was too vague? (scope not specific enough)
   - Agent over-optimized? (exceeded scope)
   - Misunderstood instructions? (clarity issue)

3. Decide on action:
   Option A: Accept if output is valid and beneficial
   Option B: Revert and redeploy with clearer scope
   Option C: Partially accept (keep good parts, redo others)

4. For this case:
   "Agent exceeded scope but improvements are valid.
    Accepting expanded changes.
    Note: Future briefs will emphasize scope boundaries more clearly."

5. Update remaining agents:
   Adjust downstream agent briefs to account for expanded changes
```

#### Edge Case 4: Parallel Agents Conflict

**Situation**: Two parallel agents modified same file, creating conflict.

**Handling:**
```
1. Detect conflict:
   "Merge conflict detected:
    Agent B modified lib/config.ts (lines 20-30)
    Agent C modified lib/config.ts (lines 25-35)
    Overlap: lines 25-30"

2. Analyze conflict:
   - Can changes coexist? (different sections ‚Üí merge safe)
   - Are changes incompatible? (same lines ‚Üí need resolution)

3. Resolution strategy:
   If coexist:
     Manually merge changes (verify no logical conflicts)

   If incompatible:
     Deploy resolution agent:
       Agent X (task-implementor):
         Scope: Reconcile conflicting config changes
         Context: Both versions of changes
         Instructions: Merge both features, resolve conflicts

4. Prevent future conflicts:
   Lesson: "Validate file-level independence more strictly"
   Action: During planning, explicitly list files per agent
```

### Common Failures and Prevention

#### Failure 1: Insufficient Agent Briefing

**Symptom**: Agent asks many questions or produces incorrect output.

**Root cause**: Briefing lacked critical information or context.

**Prevention:**
```
Briefing completeness checklist:
  ‚úÖ Task scope: Exactly what to do (1-2 sentences)
  ‚úÖ Context files: What to read with focus areas
  ‚úÖ Background: Relevant decisions/constraints (brief)
  ‚úÖ Instructions: Specific directives (3-5 bullets)
  ‚úÖ Expected output: Files to create/modify, format
  ‚úÖ Validation: Success criteria
  ‚úÖ Constraints: What NOT to do (important!)
  ‚úÖ Patterns: Examples to follow (if applicable)

Bad brief:
  "Implement caching for search"

Good brief:
  "Add Redis caching to vector search function in lib/vector-search.ts.
   Cache query results with 5-min TTL. Key format: hash(query+filters).
   Maintain existing function signature (no breaking changes).
   Use error handling pattern from lib/cache-utils.ts.
   Create helper module lib/vector-search-cache.ts.
   Update tests to mock cache calls.
   Expected: Modified lib/vector-search.ts + new lib/vector-search-cache.ts + tests."
```

#### Failure 2: Lost Context During Handoffs

**Symptom**: Sequential agent fails because it doesn't have information from previous agent.

**Root cause**: Handoff information wasn't explicitly passed.

**Prevention:**
```
Handoff execution pattern:
1. Agent A completes
2. Explicitly extract handoff info:
   - Read relevant output files
   - Summarize key points (3-5 bullets)
   - List deliverable locations
3. Brief Agent B with handoff info:
   "Agent A completed X. Deliverables: [files].
    Key points for your task: [bullets].
    Your task: [scope].
    Context: [handoff info + files to read]."
4. Do NOT assume Agent B will discover handoff info

Template:
  "Previous agent (Agent A) completed [task].
   Deliverables: [file list]
   Key information for your task:
     - [Point 1]
     - [Point 2]
     - [Point 3]
   Your task: [specific scope]
   Context to read: [specific files/sections]"
```

#### Failure 3: Parallel Execution Bottleneck

**Symptom**: One parallel agent takes much longer than others, negating parallelization benefit.

**Root cause**: Scope imbalance - agents not evenly sized.

**Prevention:**
```
Load balancing for parallel agents:
1. During planning, estimate each agent's time
2. If estimates vary significantly (>2x difference), rebalance:

   Before:
   Agent A: 10 minutes
   Agent B: 15 minutes
   Agent C: 45 minutes  ‚Üê Bottleneck
   Total: 45 minutes

   After rebalancing:
   Agent A: 10 minutes
   Agent B: 15 minutes
   Agent C1: 20 minutes (split C)
   Agent C2: 20 minutes (split C)
   Total: 20 minutes (then sequential C2: 20 min) = 40 minutes total

   Or: Run C1, C2 after A, B complete (still better)

Rule: In parallel wave, no agent should be >2x longest other agent
```

### Quality Standards for Execution Phase

**Phase complete when:**

- ‚úÖ All planned agents deployed
- ‚úÖ All agents completed (or handled failures)
- ‚úÖ Outputs collected and organized
- ‚úÖ Handoffs executed successfully (sequential)
- ‚úÖ No unresolved conflicts (parallel)
- ‚úÖ Preliminary validation passed (agents self-validated)

**Quality check questions:**
```
1. Did each agent complete its assigned scope? (If no: incomplete execution)
2. Were handoffs successful? (If no: information loss)
3. Are all deliverables present? (If no: missing outputs)
4. Did any agents exceed scope significantly? (If yes: scope creep)
5. Were conflicts detected and resolved? (If unresolved: validation will fail)
```

### Tool Usage Recommendations

**Primary tools for Phase 3:**

| Tool | Use Case | Example |
|------|----------|---------|
| `Task` | Deploy sub-agents | `Task(subagent_type="task-implementor", description="Implement caching", prompt="[detailed brief]")` |
| `Read` | Extract handoff info | `Read("query-analysis.md")` to extract top queries |
| `Bash` | Monitor progress | `Bash("ls -lt lib/")` to see recent file changes |

**Deployment patterns:**

**Parallel deployment (single message):**
```
Deploy all agents in parallel by using multiple Task tool calls in one message:
- Task(agent A)
- Task(agent B)
- Task(agent C)
[All deploy simultaneously]
```

**Sequential deployment (multiple messages):**
```
Message 1: Task(agent A)
[Wait for response]
Message 2: Extract handoff, Task(agent B)
[Wait for response]
Message 3: Extract handoff, Task(agent C)
[And so on...]
```

### Decision Points

#### Decision 1: Continue or Abort After Agent Failure

**Continue if:**
- ‚úÖ Failure is isolated (other agents unaffected)
- ‚úÖ Retry likely to succeed
- ‚úÖ Partial completion has value
- ‚úÖ Remaining work is substantial

**Abort if:**
- ‚ùå Failure invalidates entire orchestration
- ‚ùå Multiple retry attempts failed
- ‚ùå Fundamental assumption wrong
- ‚ùå User requests cancellation

**Example**: Agent 2 of 5 fails ‚Üí Continue (retry Agent 2, others can proceed)
**Example**: Agent 1 of 5 fails (foundation) ‚Üí Consider abort (others depend on it)

#### Decision 2: Provide Additional Context vs Redeploy

**Provide additional context when:**
- Agent asks clarifying question
- Agent on right track but needs guidance
- Quick context addition will unblock

**Redeploy when:**
- Agent went wrong direction
- Brief was fundamentally inadequate
- Scope needs adjustment
- Faster to restart than correct

**Rule**: If agent <30% done and off-track ‚Üí redeploy. If >70% done ‚Üí provide context to finish.

#### Decision 3: Intervene or Let Agent Finish

**Intervene when:**
- Agent clearly going wrong direction
- Agent exceeding scope significantly
- Agent will violate constraints
- Agent stuck in loop

**Let finish when:**
- Agent making progress (even if slow)
- Approach is valid (even if not expected)
- Near completion
- Uncertainty about whether approach is wrong

**Rule**: Don't intervene prematurely. Give agent chance to complete. Intervene only if confident there's a problem.

---

## Phase 4: Validate (SYNTHESIZE & VERIFY)

### Purpose

Synthesize sub-agent outputs into coherent result, verify completeness and quality, run comprehensive validation, identify gaps or issues, and ensure all requirements met.

### Extended Examples

#### Example 1: Parallel Agent Output Synthesis

**Scenario**: 4 validation utilities completed (from execution example).

**Validation steps:**

1. **Collect all outputs:**
   ```
   Agent A: lib/validators/email.ts, lib/validators/email.test.ts
   Agent B: lib/validators/phone.ts, lib/validators/phone.test.ts
   Agent C: lib/validators/card.ts, lib/validators/card.test.ts
   Agent D: lib/validators/address.ts, lib/validators/address.test.ts

   Total: 8 files
   ```

2. **Verify completeness:**
   ```
   ‚úÖ All 4 agents completed their scopes
   ‚úÖ All expected files created
   ‚úÖ No missing deliverables
   ```

3. **Check for conflicts:**
   ```
   ‚úÖ No file conflicts (each agent wrote to different files)
   ‚úÖ No type name conflicts (run Grep for duplicate exports)
   ‚úÖ No import conflicts
   ‚Üí Clean merge, no manual reconciliation needed
   ```

4. **Synthesize outputs:**
   ```
   No synthesis needed - outputs are independent modules.
   Ready for validation gates.
   ```

5. **Run validation gates:**
   ```
   Bash("pnpm format") ‚Üí ‚úÖ All files formatted
   Bash("pnpm lint") ‚Üí ‚úÖ No linting errors
   Bash("pnpm type-check") ‚Üí ‚úÖ TypeScript validates
   Bash("pnpm test validators") ‚Üí ‚úÖ All tests pass (32 tests, 0 failures)

   Check coverage:
   Bash("pnpm test:coverage validators") ‚Üí ‚úÖ 87% average coverage
   ```

6. **Verify quality standards:**
   ```
   ‚úÖ Format: All files formatted per project style
   ‚úÖ Lint: No linting violations
   ‚úÖ Types: Full type safety, no `any` types
   ‚úÖ Tests: All tests pass, coverage >80%
   ‚úÖ Documentation: Functions have docstrings
   ‚úÖ Patterns: Consistent with existing validators
   ```

7. **Evidence of completion:**
   ```
   ‚úÖ 8 files created as expected
   ‚úÖ Test output shows 32 passing tests
   ‚úÖ Coverage report confirms ‚â•80% coverage
   ‚úÖ All quality gates passed
   ‚Üí Validation complete, ready to report
   ```

#### Example 2: Sequential Pipeline Output Synthesis

**Scenario**: Query optimization pipeline completed (from execution example).

**Validation steps:**

1. **Collect outputs from each phase:**
   ```
   Agent A: query-analysis.md (identified slow queries)
   Agent B: Modified lib/users.ts, lib/posts.ts, migrations/add-index.sql
   Agent C: benchmark-results.md (performance metrics)
   Agent D: Updated docs/performance.md
   ```

2. **Verify completeness of pipeline:**
   ```
   Phase 1 (Analysis): ‚úÖ query-analysis.md present, identifies 5 slow queries
   Phase 2 (Implementation): ‚úÖ Top 3 queries optimized as planned
   Phase 3 (Benchmarking): ‚úÖ Performance improvements verified
   Phase 4 (Documentation): ‚úÖ Performance docs updated

   Pipeline complete: ‚úÖ All phases delivered expected outputs
   ```

3. **Synthesize into coherent story:**
   ```
   Analysis identified 5 slow queries (850ms, 650ms, 550ms, 380ms, 320ms).
   Optimized top 3 queries:
     - Query 1: 850ms ‚Üí 95ms (N+1 problem fixed)
     - Query 2: 650ms ‚Üí 48ms (index added)
     - Query 3: 550ms ‚Üí 215ms (join optimized)

   Total improvement: ~1.5 seconds saved per request cycle.
   Documentation updated to reflect new performance characteristics.
   ```

4. **Verify end-to-end:**
   ```
   Read lib/users.ts ‚Üí ‚úÖ N+1 fix implemented correctly (JOIN used)
   Read lib/posts.ts ‚Üí ‚úÖ Queries optimized as described
   Read migrations/add-index.sql ‚Üí ‚úÖ Index migration present
   Read benchmark-results.md ‚Üí ‚úÖ Metrics confirm improvements
   Read docs/performance.md ‚Üí ‚úÖ Documentation updated with new metrics
   ```

5. **Run validation gates:**
   ```
   Bash("pnpm format") ‚Üí ‚úÖ Formatted
   Bash("pnpm lint") ‚Üí ‚úÖ No errors
   Bash("pnpm type-check") ‚Üí ‚úÖ Types valid
   Bash("pnpm test queries") ‚Üí ‚úÖ All query tests pass

   Performance validation:
   Bash("pnpm test:performance queries") ‚Üí ‚úÖ Meets <100ms target
   ```

6. **Check for gaps:**
   ```
   ‚úÖ All top 3 queries optimized (scope met)
   ‚ö†Ô∏è  Queries 4 and 5 not optimized (expected, lower priority)
   ‚úÖ Migration provided for database changes
   ‚úÖ Tests pass (no regressions)
   ‚úÖ Documentation reflects changes

   No critical gaps identified.
   ```

7. **Evidence of completion:**
   ```
   ‚úÖ Performance metrics demonstrate success (quantified improvements)
   ‚úÖ All tests pass (no functional regressions)
   ‚úÖ Code review shows correct optimizations applied
   ‚úÖ Documentation updated (completeness verified)
   ‚Üí Validation complete
   ```

#### Example 3: Hybrid Execution Output Synthesis

**Scenario**: i18n implementation completed (from execution example).

**Validation steps:**

1. **Collect outputs from all waves:**
   ```
   Wave 1 (Foundation):
     Agent A: lib/i18n.ts, locales/en.json, locales/es.json, modified app/layout.tsx

   Wave 2 (Parallel implementation):
     Agent B: 15 modified components, updated locale files
     Agent C: 8 modified API routes, updated locale files
     Agent D: 5 modified email templates, updated locale files

   Wave 3 (Validation):
     Agent E: i18n-consistency-report.md
     Agent F: docs/i18n-guide.md, updated README.md
   ```

2. **Synthesize multi-wave outputs:**
   ```
   Infrastructure created: i18n system with 2 locales (en, es)

   Applied across application:
     - UI: 15 components internationalized
     - API: 8 endpoints return localized messages
     - Email: 5 templates support localization

   Total strings externalized: ~150 strings

   Validation: 98% strings externalized (3 exceptions documented)
   Documentation: Complete usage guide created
   ```

3. **Verify integration across waves:**
   ```
   Check: Do all waves use consistent i18n API?
     Read 5 random components ‚Üí ‚úÖ All use useTranslation() hook
     Read 3 API routes ‚Üí ‚úÖ All use i18n.t() function
     Read 2 email templates ‚Üí ‚úÖ All use i18n helpers

   Check: Are locale files complete?
     Read locales/en.json ‚Üí ‚úÖ Contains UI + API + email strings
     Read locales/es.json ‚Üí ‚úÖ All keys present (parallel to en.json)
     Validate structure ‚Üí ‚úÖ Nested structure consistent
   ```

4. **Check for conflicts/inconsistencies:**
   ```
   Grep for hardcoded strings:
   Grep('"[A-Z][a-z]+ [a-z]+"', glob="**/*.{ts,tsx}")
   ‚Üí Found 3 instances (all in error handlers, documented by Agent E)

   Check translation key naming:
     Consistent pattern: section.component.key
     ‚úÖ All keys follow pattern (validated by Agent E)
   ```

5. **Run validation gates:**
   ```
   Bash("pnpm format") ‚Üí ‚úÖ
   Bash("pnpm lint") ‚Üí ‚úÖ
   Bash("pnpm type-check") ‚Üí ‚úÖ
   Bash("pnpm test") ‚Üí ‚úÖ All tests pass

   Manual validation:
   - Load app in browser, switch to Spanish ‚Üí ‚úÖ UI translates correctly
   - Test API with locale parameter ‚Üí ‚úÖ Returns Spanish messages
   - Generate test email in Spanish ‚Üí ‚úÖ Email content in Spanish
   ```

6. **Verify requirements met:**
   ```
   Original requirement: Add multi-language support

   ‚úÖ Infrastructure: i18n system created
   ‚úÖ Coverage: UI, API, emails all internationalized
   ‚úÖ Quality: Consistent patterns, proper usage
   ‚úÖ Documentation: Usage guide complete
   ‚úÖ Testing: Manual validation confirms functionality

   Requirements fully met.
   ```

7. **Evidence of completion:**
   ```
   ‚úÖ Agent E validation report (quantified: 98% strings externalized)
   ‚úÖ Manual testing confirms translation works
   ‚úÖ All quality gates passed
   ‚úÖ Documentation complete
   ‚úÖ Locale files contain all strings
   ‚Üí Validation complete
   ```

### Edge Cases and Handling

#### Edge Case 1: Conflicting Outputs from Parallel Agents

**Situation**: Two agents modified same file differently, changes conflict.

**Handling:**
```
1. Identify conflict:
   Read lib/config.ts from Agent B ‚Üí Added cache config (lines 20-30)
   Read lib/config.ts from Agent C ‚Üí Added auth config (lines 20-35)
   Conflict: Both modified overlapping lines

2. Analyze compatibility:
   - Are changes logically compatible? (Both add config sections ‚Üí YES)
   - Can they coexist? (Different config keys ‚Üí YES)
   - Is merge safe? (No logic conflicts ‚Üí YES)

3. Merge strategy:
   Manually merge:
     Lines 20-30: Cache config (from Agent B)
     Lines 31-45: Auth config (from Agent C, adjusted line numbers)

   Result: lib/config.ts contains both cache and auth config

4. Validate merged version:
   Bash("pnpm type-check") ‚Üí ‚úÖ Types valid
   Bash("pnpm test config") ‚Üí ‚úÖ Tests pass

   Merged version validated.

5. Document resolution:
   "Agents B and C both modified lib/config.ts.
    Manually merged both changes (cache + auth config).
    Validation confirms merge is correct."
```

#### Edge Case 2: Output Missing from Agent

**Situation**: Agent reported completion but expected file doesn't exist.

**Handling:**
```
1. Verify claim:
   Agent D claimed: "Created docs/api-guide.md"
   Check: Bash("ls docs/api-guide.md") ‚Üí File not found

2. Investigate:
   - Agent error? (misreported completion)
   - File location wrong? (created in wrong directory)
   - Naming difference? (api-reference.md vs api-guide.md)

3. Search for file:
   Glob("**/api-*.md") ‚Üí Found docs/api-reference.md (not api-guide.md)
   Read docs/api-reference.md ‚Üí Contains expected content

4. Resolution:
   Agent created file with different name than claimed.
   Expected: docs/api-guide.md
   Actual: docs/api-reference.md

   Action: Accept actual file (content is correct)
   Note: "Agent D created docs/api-reference.md (not api-guide.md as claimed)"

5. Decide: Rename or keep?
   Check specs: No requirement for specific name
   Decision: Keep as api-reference.md (more conventional name)
```

#### Edge Case 3: Validation Gate Fails

**Situation**: Agent completed but tests fail.

**Handling:**
```
1. Identify failure:
   Bash("pnpm test") ‚Üí 3 tests failing in lib/search.test.ts

2. Analyze failures:
   Read test output:
     - Test "should return empty for no results" fails
     - Test "should handle special characters" fails
     - Test "should validate input" fails

   All failures in search module (Agent B's scope)

3. Investigate root cause:
   Read lib/search.ts (Agent B's changes) ‚Üí Implementation correct
   Read lib/search.test.ts ‚Üí Tests not updated for new behavior

   Root cause: Agent B changed function signature but didn't update tests

4. Resolution options:
   Option A: Redeploy Agent B to fix tests
   Option B: Fix tests manually (if trivial)
   Option C: Deploy new agent to fix tests

   Choose: Option B (3 simple test updates)

5. Fix and re-validate:
   Edit lib/search.test.ts ‚Üí Update 3 test calls with new signature
   Bash("pnpm test") ‚Üí ‚úÖ All tests pass

   Validation now passes, can proceed.

6. Document:
   "Agent B implementation correct but tests needed updates.
    Manually updated 3 tests to match new function signature.
    All tests now passing."
```

#### Edge Case 4: Incomplete Coverage of Requirements

**Situation**: Agents completed but some requirements not met.

**Handling:**
```
1. Compare deliverables to requirements:
   Requirement: "Add filters for price, category, and brand"

   Delivered:
     ‚úÖ Price filter implemented
     ‚úÖ Category filter implemented
     ‚ùå Brand filter missing

2. Investigate why:
   Review Agent C scope: "Implement price and category filters"
   ‚Üí Agent completed its assigned scope
   ‚Üí Brand filter was missed during planning

3. Determine criticality:
   Ask user: "Brand filter was not implemented. Is this required for completion?"

   If critical: Deploy additional agent for brand filter
   If optional: Document as follow-up work

4. Resolution (if critical):
   Deploy Agent E (task-implementor):
     Scope: Add brand filter
     Context: Existing price and category filters
     Pattern: Follow same pattern as other filters

   [Wait for completion]

   Re-validate with brand filter included.

5. Document:
   "Initial planning missed brand filter requirement.
    Deployed Agent E to complete brand filter.
    All filters now implemented and validated."
```

### Common Failures and Prevention

#### Failure 1: Skipping Validation Gates

**Symptom**: Proceed to reporting phase without running tests/linting/formatting.

**Root cause**: Assume agents validated their own work, skip comprehensive validation.

**Prevention:**
```
Mandatory validation checklist:
  ‚úÖ Format check: pnpm format:check (or pnpm format)
  ‚úÖ Lint check: pnpm lint
  ‚úÖ Type check: pnpm type-check
  ‚úÖ Tests: pnpm test [relevant scope]
  ‚úÖ Coverage: pnpm test:coverage (if applicable)
  ‚úÖ Build: pnpm build (if applicable)
  ‚úÖ Project-specific: Any custom validation (benchmarks, integration tests)

NEVER skip these gates. They catch issues agents missed.
```

**Recovery**: Run validation gates before finalizing. Fix any failures found.

#### Failure 2: Not Verifying Completeness

**Symptom**: Proceed without checking if all requirements met.

**Root cause**: Trust agents completed work without verification.

**Prevention:**
```
Completeness verification:
1. List original requirements (from Phase 1)
2. List agent deliverables (from Phase 3)
3. Map each requirement to deliverable
4. Identify gaps:
   ‚úÖ Requirement met
   ‚ö†Ô∏è  Partially met (document gap)
   ‚ùå Not met (critical issue)

If ANY requirement not met ‚Üí Investigate and address before reporting.
```

**Recovery**: Identify missing work, deploy additional agent or document as limitation.

#### Failure 3: Accepting Poor Quality Output

**Symptom**: Agent output works but violates quality standards (no tests, hardcoded values, poor naming).

**Root cause**: Validate only functionality, ignore code quality.

**Prevention:**
```
Quality standards checklist:
  ‚úÖ Code follows project conventions (naming, structure)
  ‚úÖ No hardcoded values (use config/constants)
  ‚úÖ No placeholder comments ("TODO", "FIXME")
  ‚úÖ Tests present and meaningful (not trivial)
  ‚úÖ Coverage meets target (‚â•80% for modified code)
  ‚úÖ Documentation present (docstrings, comments for complex logic)
  ‚úÖ Error handling appropriate (no silent failures)
  ‚úÖ Types explicit (no excessive `any` usage)

If quality issues found ‚Üí Fix before reporting (manual edits or redeploy agent).
```

**Recovery**: Address quality issues through manual fixes or additional agent deployment.

### Quality Standards for Validation Phase

**Phase complete when:**

- ‚úÖ All agent outputs collected and organized
- ‚úÖ Completeness verified (all requirements met)
- ‚úÖ Conflicts resolved (if any)
- ‚úÖ Outputs synthesized into coherent result
- ‚úÖ All validation gates passed (format|lint|types|tests)
- ‚úÖ Quality standards met (coverage, documentation, conventions)
- ‚úÖ Evidence of completion documented
- ‚úÖ No critical issues remaining

**Quality check questions:**
```
1. Did all agents produce expected deliverables? (If no: investigate gaps)
2. Do validation gates pass? (If no: fix issues)
3. Are requirements fully met? (If no: deploy additional work)
4. Is code quality acceptable? (If no: refine outputs)
5. Is there evidence of correctness? (If no: add validation)
6. Can I confidently report success? (If no: identify what's missing)
```

### Tool Usage Recommendations

**Primary tools for Phase 4:**

| Tool | Use Case | Example |
|------|----------|---------|
| `Read` | Review agent outputs | `Read("lib/feature.ts")` |
| `Grep` | Search for issues | `Grep("TODO", output_mode="files_with_matches")` |
| `Bash` | Run validation gates | `Bash("pnpm test && pnpm lint && pnpm type-check")` |
| `Glob` | Find deliverables | `Glob("lib/**/*.test.ts")` to find all tests |

**Validation pattern:**
```
1. Collect outputs: Use Read/Glob to gather all deliverables
2. Check completeness: Map deliverables to requirements
3. Run gates: Use Bash to run project validation commands
4. Verify quality: Use Read to manually inspect critical files
5. Document evidence: Note all validation results
```

### Decision Points

#### Decision 1: Accept or Reject Agent Output

**Accept when:**
- ‚úÖ Deliverables match expectations
- ‚úÖ Validation gates pass
- ‚úÖ Quality standards met
- ‚úÖ No critical issues

**Reject when:**
- ‚ùå Missing deliverables
- ‚ùå Validation failures
- ‚ùå Poor quality (violates standards)
- ‚ùå Doesn't meet requirements

**Partial accept when:**
- ‚ö†Ô∏è  Most work is good but has fixable issues
- ‚ö†Ô∏è  Core functionality present, refinement needed
- Fix minor issues manually or with small agent deployment

#### Decision 2: Fix Issues Manually or Redeploy Agent

**Fix manually when:**
- Issue is trivial (typo, formatting, simple logic fix)
- Fix takes <5 minutes
- High confidence in correctness
- Redeploy would take longer

**Redeploy agent when:**
- Issue is complex (requires design thought)
- Multiple related issues (systematic problem)
- Uncertain about correct fix
- Want agent to validate its own fix

**Rule**: Manual fixes for <5 min issues. Redeploy for anything more complex.

#### Decision 3: Continue or Deploy Additional Agents

**Continue to reporting when:**
- ‚úÖ All requirements met
- ‚úÖ All validation passes
- ‚úÖ Quality acceptable
- ‚úÖ No critical gaps

**Deploy additional agents when:**
- ‚ùå Requirement not met (scope gap)
- ‚ùå Quality issues need dedicated agent (e.g., test coverage too low)
- ‚ùå Integration work needed (outputs need coordination)
- ‚ùå Documentation gaps (need documentation-manager)

**Example**: All code done but no docs ‚Üí Deploy documentation-manager agent.

---

## Phase 5: Confirm (REPORT & DOCUMENT)

### Purpose

Generate comprehensive synthesis report, document orchestration results, update project documentation, communicate outcomes to user, and prompt for next actions.

### Extended Examples

#### Example 1: Simple Parallel Orchestration Report

**Scenario**: 4 validation utilities completed and validated.

**Reporting steps:**

1. **Generate synthesis report:**
   ```
   üìä Multi-Agent Orchestration Complete

   ## Deployed Agents
   - Agent A (task-implementor): Email validation ‚úÖ
   - Agent B (task-implementor): Phone validation ‚úÖ
   - Agent C (task-implementor): Credit card validation ‚úÖ
   - Agent D (task-implementor): Address validation ‚úÖ

   ## Deliverables
   Created 8 files:
   - lib/validators/email.ts (email validation with regex patterns)
   - lib/validators/email.test.ts (18 tests, 92% coverage)
   - lib/validators/phone.ts (international phone validation)
   - lib/validators/phone.test.ts (15 tests, 88% coverage)
   - lib/validators/card.ts (credit card number validation)
   - lib/validators/card.test.ts (12 tests, 85% coverage)
   - lib/validators/address.ts (address format validation)
   - lib/validators/address.test.ts (14 tests, 90% coverage)

   ## Validation Results
   ‚úÖ Format check: PASS
   ‚úÖ Lint check: PASS
   ‚úÖ Type check: PASS
   ‚úÖ Tests: PASS (59 tests, 0 failures)
   ‚úÖ Coverage: 89% average (target: 80%)

   ## Changes Made
   - Files created: 8
   - Tests added: 59
   - Lines of code: ~850

   ## Execution Summary
   Execution strategy: Parallel (all agents simultaneously)
   Total time: 25 minutes
   Agents deployed: 4
   Success rate: 100%

   ## Next Steps
   - Import validators in application code
   - Add validators to form validation pipeline
   - Consider adding more validator types (URL, date, etc.)
   ```

2. **Update project documentation (if applicable):**
   ```
   No documentation updates needed - validators are self-documenting
   with TypeScript types and docstrings.
   ```

3. **Prompt user for next action:**
   ```
   Orchestration complete. What would you like to do next?
   - Continue: Integrate validators into application
   - Review: Examine specific validator implementation
   - Test: See example usage
   - Document: Create usage guide
   - Exit: Orchestration finished
   ```

#### Example 2: Complex Sequential Pipeline Report

**Scenario**: Query optimization pipeline completed.

**Reporting steps:**

1. **Generate synthesis report:**
   ```
   üìä Multi-Agent Orchestration Complete

   ## Deployed Agents
   - Agent A (Explore): Query analysis ‚úÖ
   - Agent B (task-implementor): Query optimization ‚úÖ
   - Agent C (systematic-debugger): Performance benchmarking ‚úÖ
   - Agent D (documentation-manager): Documentation update ‚úÖ

   ## Deliverables

   Analysis:
   - query-analysis.md (profiling report, identified 5 slow queries)

   Implementation:
   - lib/users.ts (optimized user query - N+1 fix)
   - lib/posts.ts (optimized 2 post queries - index + join optimization)
   - migrations/add-post-index.sql (database migration for new index)

   Validation:
   - benchmark-results.md (performance metrics before/after)

   Documentation:
   - docs/performance.md (updated with optimization case study)

   ## Validation Results
   ‚úÖ Format check: PASS
   ‚úÖ Lint check: PASS
   ‚úÖ Type check: PASS
   ‚úÖ Tests: PASS (all query tests pass, no regressions)
   ‚úÖ Performance: PASS (all targets met)

   ## Performance Improvements
   Query 1 (users): 850ms ‚Üí 95ms (9x faster, N+1 eliminated)
   Query 2 (posts): 650ms ‚Üí 48ms (13.5x faster, index added)
   Query 3 (posts): 550ms ‚Üí 215ms (2.6x faster, join optimized)

   Total improvement: ~1.5 seconds saved per request cycle

   ## Changes Made
   - Files modified: 3 (lib/users.ts, lib/posts.ts, docs/performance.md)
   - Files created: 3 (query-analysis.md, benchmark-results.md, migrations/add-post-index.sql)
   - Database migrations: 1 (index creation)
   - Performance improvements: 3 queries optimized

   ## Execution Summary
   Execution strategy: Sequential (4-phase pipeline)
   Total time: 68 minutes
   Phases: Analysis(12m) ‚Üí Implementation(28m) ‚Üí Benchmarking(18m) ‚Üí Documentation(10m)
   Success rate: 100%

   ## Next Steps
   - Run database migration: `pnpm db:migrate`
   - Deploy optimizations to production
   - Monitor query performance in production
   - Consider optimizing remaining 2 queries (lower priority)
   ```

2. **Update project documentation:**
   ```
   Documentation already updated by Agent D:
   - docs/performance.md now includes query optimization case study
   - Benchmark results documented
   - Performance expectations updated

   No additional documentation needed.
   ```

3. **Document decision log (if significant decisions made):**
   ```
   No architectural decisions made. Implementation followed standard
   optimization patterns (eliminate N+1, add indexes, optimize joins).
   Decision log update not required.
   ```

4. **Prompt user for next action:**
   ```
   Orchestration complete. Database migration pending.

   Next steps:
   1. Run migration: `pnpm db:migrate` (creates index)
   2. Restart development server
   3. Verify query improvements in development

   What would you like to do?
   - Run migration now
   - Review optimization details
   - Test in development environment
   - Commit changes
   - Exit
   ```

#### Example 3: Large Hybrid Orchestration Report

**Scenario**: i18n implementation completed.

**Reporting steps:**

1. **Generate comprehensive synthesis report:**
   ```
   üìä Multi-Agent Orchestration Complete

   ## Deployed Agents (6 agents, 3 waves)

   Wave 1 - Foundation:
   - Agent A (task-implementor): i18n infrastructure ‚úÖ

   Wave 2 - Implementation (parallel):
   - Agent B (task-implementor): UI components i18n ‚úÖ
   - Agent C (task-implementor): API messages i18n ‚úÖ
   - Agent D (task-implementor): Email templates i18n ‚úÖ

   Wave 3 - Validation:
   - Agent E (principle-evaluator): Consistency validation ‚úÖ
   - Agent F (documentation-manager): Documentation ‚úÖ

   ## Deliverables

   Infrastructure:
   - lib/i18n.ts (i18n hook, provider, utilities)
   - locales/en.json (English translations - 158 strings)
   - locales/es.json (Spanish translations - 158 strings)
   - app/layout.tsx (added i18n provider)

   UI Internationalization:
   - 15 component files modified (components/**/*.tsx)
   - All UI text externalized using useTranslation() hook

   API Internationalization:
   - 8 API route files modified (app/api/**/*.ts)
   - All response messages externalized using i18n.t()

   Email Internationalization:
   - 5 email template files modified (lib/email/**/*.ts)
   - All email content supports localization

   Validation & Documentation:
   - i18n-consistency-report.md (validation results)
   - docs/i18n-guide.md (usage guide for developers)
   - README.md (added i18n section)

   ## Validation Results
   ‚úÖ Format check: PASS
   ‚úÖ Lint check: PASS
   ‚úÖ Type check: PASS
   ‚úÖ Tests: PASS (all tests pass, i18n doesn't break functionality)
   ‚úÖ String externalization: 98% (3 exceptions documented)
   ‚úÖ Manual testing: PASS (UI, API, emails all translate correctly)

   ## Changes Made
   - Files created: 5 (i18n infrastructure + docs)
   - Files modified: 29 (15 components + 8 API routes + 5 emails + 1 layout)
   - Strings externalized: 158
   - Languages supported: 2 (English, Spanish)
   - Translation coverage: 100% (all keys translated)

   ## Execution Summary
   Execution strategy: Hybrid (sequential ‚Üí parallel ‚Üí sequential)
   Total time: 77 minutes
   Wave 1: 25 minutes (foundation)
   Wave 2: 30 minutes (parallel implementation)
   Wave 3: 22 minutes (validation + docs)
   Agents deployed: 6
   Success rate: 100%

   ## Quality Notes
   - 98% strings externalized (3 exceptions in error handlers - acceptable)
   - Consistent translation key naming (section.component.key pattern)
   - All locale files use nested JSON structure (maintainable)
   - Documentation includes usage examples and guidelines

   ## Known Limitations
   - 3 error handler strings remain hardcoded (technical limitation documented)
   - Only 2 languages supported initially (more can be added easily)
   - Date/time formatting not yet localized (future enhancement)

   ## Next Steps
   - Test application in both languages (English/Spanish)
   - Add more language support if needed (French, German, etc.)
   - Consider adding date/time localization
   - Deploy to staging for QA testing
   ```

2. **Update project documentation:**
   ```
   Agent F already updated:
   - docs/i18n-guide.md (new developer guide)
   - README.md (added i18n section)

   Additional recommended updates:
   - specs/constitution.md: Add i18n as project standard
   - specs/spec.md: Update to reflect i18n requirement

   Would you like me to update these files? (y/n)
   ```

3. **Document decision log:**
   ```
   Significant architectural decision made:

   Decision: Use client-side i18n with React hooks
   Rationale: Next.js 15 App Router with Server Components
   Alternatives considered:
     - Server-side i18n (rejected: complex with React Server Components)
     - Route-based i18n (rejected: adds complexity to routing)
   Impact:
     - Translation happens client-side (acceptable performance)
     - Locale state managed in React context
     - SEO: Can add server-side metadata translation later if needed

   Would you like me to document this in project decision log? (y/n)
   ```

4. **Prompt user for next action:**
   ```
   Orchestration complete. i18n fully implemented.

   Recommended next steps:
   1. Test application:
      - Load app, switch to Spanish, verify UI translates
      - Test API endpoints with locale parameter
      - Generate test emails in both languages

   2. Deploy to staging for QA testing

   3. Add more languages (if needed):
      - Create locales/[lang].json
      - Copy key structure from en.json
      - Translate strings

   4. Commit changes:
      - 34 files changed
      - Recommend commit message: "feat: add i18n support for UI, API, and emails"

   What would you like to do next?
   - Test: Manually test i18n functionality
   - Expand: Add more languages
   - Review: Examine specific implementation
   - Commit: Commit all changes
   - Document: Update specs with i18n decision
   - Exit: Orchestration finished
   ```

### Edge Cases and Handling

#### Edge Case 1: Orchestration Partially Failed

**Situation**: Some agents succeeded, some failed, proceeded with partial completion.

**Handling:**
```
üìä Multi-Agent Orchestration: Partial Completion

## Status: ‚ö†Ô∏è  PARTIAL SUCCESS

## Deployed Agents
- Agent A (Explore): Analysis ‚úÖ
- Agent B (task-implementor): Module 1 implementation ‚úÖ
- Agent C (task-implementor): Module 2 implementation ‚ùå FAILED
- Agent D (documentation-manager): Not deployed (blocked by C)

## Completed Work

Successfully delivered:
- query-analysis.md (analysis report)
- lib/module1.ts (implementation)
- lib/module1.test.ts (tests, 85% coverage)

## Failed Components

Agent C failure:
- Scope: Implement Module 2
- Error: Context overflow during execution
- Root cause: Module 2 scope too large for single agent
- Status: Not completed

Agent D:
- Status: Not deployed (depends on Agent C)
- Scope: Update documentation

## Partial Deliverables Status
‚úÖ Analysis: Complete
‚úÖ Module 1: Complete and validated
‚ùå Module 2: Not completed
‚ùå Documentation: Not completed

## Validation Results (Partial)
‚úÖ Format check: PASS (for completed work)
‚úÖ Lint check: PASS
‚úÖ Type check: PASS
‚úÖ Tests: PASS (Module 1 tests)
‚ö†Ô∏è  Coverage: Module 1 only (Module 2 pending)

## Recommended Next Steps

Option A: Resume orchestration
- Break Agent C scope into 2 smaller agents (C1, C2)
- Deploy C1 and C2 sequentially
- Deploy Agent D after C1, C2 complete
- Estimated time: 40 minutes

Option B: Accept partial completion
- Keep Module 1 implementation
- Defer Module 2 to future work
- Update documentation for Module 1 only
- Estimated time: 15 minutes

Option C: Rollback and redesign
- Revert Module 1 changes
- Redesign orchestration with different scope
- Restart with revised plan
- Estimated time: 60+ minutes

Recommendation: Option A (resume with adjusted scope)

What would you like to do?
- Resume: Complete Module 2 with adjusted approach
- Accept: Keep Module 1, defer Module 2
- Rollback: Revert and redesign
- Review: Examine failure details
```

#### Edge Case 2: User Requests Additional Work During Reporting

**Situation**: During confirmation phase, user asks "also add feature X".

**Handling:**
```
Current orchestration complete (4 agents deployed successfully).

User request: "Also add logging to all validator functions"

This is new scope, not part of original orchestration.

Options:
1. Start new orchestration:
   - Treat as separate task
   - Plan agent deployment for logging feature
   - Estimate: 20 minutes (1 agent)

2. Extend current orchestration:
   - Deploy Agent E for logging
   - Continue current session
   - Update synthesis report after completion

3. Document as follow-up:
   - Add to task list / backlog
   - Complete current orchestration first
   - Address logging in future session

Recommendation: Option 1 (new orchestration)
Rationale: Keeps orchestrations focused, separate concern

Would you like me to:
a. Start new orchestration for logging feature now
b. Complete current orchestration and add logging to backlog
c. Extend current orchestration with logging agent
```

#### Edge Case 3: No Clear Next Steps

**Situation**: Work is complete but no obvious next action.

**Handling:**
```
üìä Multi-Agent Orchestration Complete

[... standard synthesis report ...]

## Next Steps

Work is complete. Possible follow-up actions:

Development:
- Use new validators in application forms
- Add more validator types (URL, date, etc.)
- Extend existing validators with additional rules

Quality:
- Review validator implementation details
- Add more test cases for edge cases
- Perform code review

Deployment:
- Commit changes to version control
- Create pull request for review
- Deploy to staging environment

Documentation:
- Create usage guide for validators
- Add examples to documentation
- Update API reference

Planning:
- Identify next feature to implement
- Review project backlog
- Plan next orchestration

What would you like to do?
- Review: Examine implementation details
- Test: See example usage
- Commit: Commit all changes
- Plan: Identify next task
- Exit: Orchestration finished

(Or describe specific next action)
```

#### Edge Case 4: Validation Passed But User Not Satisfied

**Situation**: All validation gates passed but user says "this isn't what I wanted".

**Handling:**
```
All validation gates passed but output doesn't meet expectations.

Let's identify the gap:
1. What's wrong with the current implementation?
   [Wait for user response]

2. What was expected instead?
   [Wait for user response]

3. Was the initial requirement clear?
   - If unclear: Requirement ambiguity caused mismatch
   - If clear: Implementation deviated from requirement

Analysis:
[Based on user response]

Options:
A. Adjust and redeploy:
   - Modify agent scopes to match expectations
   - Redeploy affected agents
   - Re-validate

B. Refine current implementation:
   - Keep core work, refine details
   - Deploy refinement agent with specific adjustments
   - Faster than full redeploy

C. Start over with clearer requirements:
   - Revert current changes
   - Clarify requirements in detail
   - Restart orchestration with adjusted plan

Based on the gap you described, I recommend: [option with rationale]

How would you like to proceed?
```

### Common Failures and Prevention

#### Failure 1: Vague or Incomplete Synthesis Report

**Symptom**: Report doesn't clearly communicate what was accomplished.

**Root cause**: Report omits key details or uses vague language.

**Prevention:**
```
Synthesis report must include:
  ‚úÖ All agents deployed (with types and scopes)
  ‚úÖ All deliverables with descriptions
  ‚úÖ Validation results (explicit pass/fail per gate)
  ‚úÖ Quantitative metrics (files, tests, coverage, performance)
  ‚úÖ Changes made (files created/modified, lines of code)
  ‚úÖ Execution summary (time, strategy, success rate)
  ‚úÖ Next steps (specific, actionable)

Bad: "Implemented search feature"
Good: "Implemented product search with price filters (components/ProductSearch.tsx),
      added 12 tests (85% coverage), all validation gates passed"
```

#### Failure 2: Not Prompting for Next Action

**Symptom**: Report ends without asking user what to do next.

**Root cause**: Assume orchestration fully complete, don't prompt for continuation.

**Prevention:**
```
Always end reporting phase with clear next action prompt:

Bad:
  "Orchestration complete."
  [End]

Good:
  "Orchestration complete. What would you like to do next?
   - Test: Manually test new feature
   - Review: Examine implementation details
   - Commit: Commit all changes
   - Continue: Implement next feature
   - Exit: Orchestration finished"

Provide specific options relevant to completed work.
```

#### Failure 3: Missing Documentation Updates

**Symptom**: Code changes complete but documentation not updated.

**Root cause**: Forget to check if documentation needs updates.

**Prevention:**
```
Documentation update checklist:
  ‚úÖ API changes ‚Üí Update API reference docs
  ‚úÖ New features ‚Üí Update user guides
  ‚úÖ Architecture changes ‚Üí Update architecture docs / decision log
  ‚úÖ Configuration changes ‚Üí Update configuration guides
  ‚úÖ Breaking changes ‚Üí Update migration guides
  ‚úÖ Performance changes ‚Üí Update performance docs

If ANY documentation should be updated ‚Üí Either:
  - Deploy documentation-manager agent (if not already done)
  - Recommend documentation updates to user
  - Document as follow-up work

Never assume documentation updates can be skipped.
```

### Quality Standards for Confirmation Phase

**Phase complete when:**

- ‚úÖ Comprehensive synthesis report generated
- ‚úÖ All deliverables documented with descriptions
- ‚úÖ Validation results explicitly stated
- ‚úÖ Changes quantified (files, lines, tests, etc.)
- ‚úÖ Documentation updated (or flagged for update)
- ‚úÖ Next steps clearly identified
- ‚úÖ User prompted for next action
- ‚úÖ Session can be closed cleanly

**Quality check questions:**
```
1. Does report clearly communicate what was accomplished? (If no: add detail)
2. Are all deliverables listed and described? (If no: complete list)
3. Are validation results explicit? (If no: add validation details)
4. Does user know what to do next? (If no: provide options)
5. Is documentation up to date? (If no: update or flag)
6. Can orchestration session close cleanly? (If no: address remaining items)
```

### Tool Usage Recommendations

**Primary tools for Phase 5:**

| Tool | Use Case | Example |
|------|----------|---------|
| `Read` | Review template | `Read("assets/synthesis-report.tmpl")` |
| `Bash` | Generate report script | `Bash("python scripts/synthesize_reports.py")` |
| `Write` | Save report to file | `Write("reports/orchestration-report.md", content)` (if needed) |

**Reporting pattern:**
```
1. Gather information from all phases
2. Structure using template or manual format
3. Include all required sections (agents, deliverables, validation, next steps)
4. Present to user
5. Prompt for next action
6. Document session outcome (if significant)
```

### Decision Points

#### Decision 1: Detailed vs Summary Report

**Detailed report when:**
- Complex orchestration (5+ agents)
- Multiple waves or phases
- Significant architectural changes
- Many deliverables

**Summary report when:**
- Simple orchestration (2-3 agents)
- Single straightforward task
- Few deliverables
- Self-evident outcomes

**Default**: Err toward more detail. Better to over-communicate than under-communicate.

#### Decision 2: Update Documentation Now or Defer

**Update now when:**
- Documentation-manager agent already deployed
- Quick updates (<5 minutes)
- Critical documentation (must be in sync with code)

**Defer when:**
- Documentation updates are extensive (>15 minutes)
- User prefers to handle documentation separately
- Documentation can wait for PR review

**Ask user**: "Documentation updates recommended. Update now or defer?"

#### Decision 3: Close Session or Continue

**Close session when:**
- All work complete
- User satisfied
- No immediate follow-up
- Natural stopping point

**Continue when:**
- User requests additional work
- Follow-up orchestration needed
- Issues discovered requiring fixes
- User wants to proceed immediately

**Prompt user**: Always ask "What would you like to do next?" Don't assume session should end.

---

## Cross-Phase Guidance

### Decision Tree: When to Escalate to User

Use this decision tree throughout all phases:

```
Question or issue arises
‚îÇ
‚îú‚îÄ Can I answer based on existing context?
‚îÇ  ‚îú‚îÄ YES ‚Üí Proceed autonomously
‚îÇ  ‚îî‚îÄ NO ‚Üí Continue down tree
‚îÇ
‚îú‚îÄ Is this a technical implementation detail?
‚îÇ  ‚îú‚îÄ YES ‚Üí Use judgment, proceed autonomously
‚îÇ  ‚îî‚îÄ NO ‚Üí Continue down tree
‚îÇ
‚îú‚îÄ Does this affect user-facing behavior or requirements?
‚îÇ  ‚îú‚îÄ YES ‚Üí Ask user
‚îÇ  ‚îî‚îÄ NO ‚Üí Continue down tree
‚îÇ
‚îú‚îÄ Are there multiple valid approaches?
‚îÇ  ‚îú‚îÄ YES ‚Üí Present options to user
‚îÇ  ‚îî‚îÄ NO ‚Üí Continue down tree
‚îÇ
‚îú‚îÄ Am I uncertain about the correct approach?
‚îÇ  ‚îú‚îÄ YES ‚Üí Ask user
‚îÇ  ‚îî‚îÄ NO ‚Üí Proceed autonomously
‚îÇ
‚îî‚îÄ Default: When in doubt, ask user
```

### Tool Selection Matrix

| Task | Best Tool | Alternative | When to Use Alternative |
|------|-----------|-------------|-------------------------|
| Find files by name | `Glob` | `Bash(ls)` | Need file metadata (dates, sizes) |
| Search code content | `Grep` | `Bash(grep)` | Never (always use Grep tool) |
| Read specific file | `Read` | - | Always for reading files |
| Deploy sub-agent | `Task` | - | Always for sub-agents |
| Run validation | `Bash` | - | Always for commands (tests, lint, etc.) |
| Explore codebase | `Task(Explore)` | `Grep` + `Read` | Small scope: manual; Large scope: agent |

### Common Anti-Patterns to Avoid

**Anti-pattern 1: Over-orchestration**
```
Bad: Deploy 10 agents for simple task
Good: Deploy 3-5 agents for complex tasks, fewer for simple tasks
Rule: If task can be done in <30 min without orchestration, don't orchestrate
```

**Anti-pattern 2: Under-specification**
```
Bad: "Agent A: Fix bugs"
Good: "Agent A (systematic-debugger): Debug failing test in lib/search.test.ts
       (test 'handles empty query'). Root cause analysis, fix, verify."
Rule: Every agent scope should be 1-2 specific sentences
```

**Anti-pattern 3: Assuming success**
```
Bad: Agent completes ‚Üí Move to next phase (no validation)
Good: Agent completes ‚Üí Verify deliverables ‚Üí Check quality ‚Üí Then proceed
Rule: Always validate before proceeding to next phase
```

**Anti-pattern 4: Context bloat**
```
Bad: Pass entire conversation history to each agent
Good: Pass only specific handoff information (file paths, summaries)
Rule: Minimal context principle - only what's needed for agent's specific task
```

**Anti-pattern 5: Silent failures**
```
Bad: Agent fails ‚Üí Proceed anyway, report success
Good: Agent fails ‚Üí Analyze ‚Üí Retry or adjust ‚Üí Report actual status
Rule: Never hide failures, always address them explicitly
```

---

**This playbook provides comprehensive phase guidance. For coordination patterns, see `references/agent-coordination.md`. For error handling, see `references/error-scenarios.md`.**

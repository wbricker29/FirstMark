# Talent Signal Agent

AI-powered executive matching for FirstMark Capital portfolio companies.

## Overview

Working directory for all information and projects related to Will Bricker's evaluation for the role of AI Lead at FirstMark Capital.

**Case Study:** Prototype AI agent for matching executives with portfolio company roles.

## Phase 1: Project Setup (‚úÖ COMPLETE)

The foundational setup has been completed. All dependencies are installed and Pydantic models are validated.

### Prerequisites

- Python 3.11+ (verified with `.python-version`)
- UV package manager (for dependency management)

### Setup Instructions

1. **Verify Python Version**
   ```bash
   python --version  # Should show Python 3.11+
   ```

2. **Install UV** (if not already installed)
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

3. **Install Dependencies**
   ```bash
   uv pip install -e .
   ```

4. **Configure Environment Variables**
   - Copy `.env.example` to `.env`
   - Populate required API keys:
     - `OPENAI_API_KEY`: OpenAI API key
     - `AIRTABLE_API_KEY`: Airtable personal access token
     - `AIRTABLE_BASE_ID`: Airtable base ID

5. **Verify Installation**
   ```bash
   # Check all packages installed
   uv pip list | grep -E "(agno|pydantic|flask|pyairtable|dotenv)"

   # Verify models can be imported
   .venv/bin/python -c "from demo.models import ExecutiveResearchResult, AssessmentResult; print('‚úÖ Models loaded')"

   # Run model validation tests
   .venv/bin/python tests/test_models_validation.py
   ```

### Project Structure

```
demo/                      # Core implementation
‚îú‚îÄ‚îÄ __init__.py           # Package initialization
‚îú‚îÄ‚îÄ app.py                # Legacy Flask webhook server (backwards compatibility)
‚îú‚îÄ‚îÄ agentos_app.py        # AgentOS FastAPI runtime (canonical)
‚îú‚îÄ‚îÄ agents.py             # Agno agent definitions (‚úÖ implemented)
‚îú‚îÄ‚îÄ models.py             # Pydantic data models (‚úÖ implemented)
‚îú‚îÄ‚îÄ airtable_client.py    # Airtable API wrapper
‚îú‚îÄ‚îÄ settings.py           # Configuration/env loading
‚îî‚îÄ‚îÄ prompts/              # ‚úÖ Centralized prompt system
    ‚îú‚îÄ‚îÄ __init__.py       # Prompt package init
    ‚îú‚îÄ‚îÄ catalog.yaml      # Agent prompt definitions (4 agents)
    ‚îî‚îÄ‚îÄ library.py        # Prompt loader (get_prompt function)

tmp/                       # Agno session database location
tests/                     # Test suite
‚îú‚îÄ‚îÄ test_models_validation.py  # ‚úÖ Model validation tests
‚îú‚îÄ‚îÄ test_scoring.py       # Dimension scoring tests (Phase 2)
‚îú‚îÄ‚îÄ test_quality_check.py # Research quality tests (Phase 2)
‚îî‚îÄ‚îÄ test_workflow_smoke.py # End-to-end tests (Phase 2)

spec/                      # Technical specifications
‚îú‚îÄ‚îÄ constitution.md       # Project governance
‚îú‚îÄ‚îÄ prd.md                # Product requirements
‚îú‚îÄ‚îÄ spec.md               # Technical spec
‚îú‚îÄ‚îÄ dev_reference/        # Implementation guides
‚îÇ   ‚îú‚îÄ‚îÄ implementation_guide.md  # Master implementation doc
‚îÇ   ‚îú‚îÄ‚îÄ AGNO_REFERENCE.md        # Framework patterns
‚îÇ   ‚îî‚îÄ‚îÄ airtable_ai_spec.md      # Database schema
‚îî‚îÄ‚îÄ units/                # Feature design docs
```

### What's Complete

- ‚úÖ Python 3.11+ environment configured
- ‚úÖ All dependencies installed via UV
- ‚úÖ Environment configuration files (.env, .env.example, .gitignore)
- ‚úÖ Project directory structure (demo/, tmp/, tests/)
- ‚úÖ All 6 Pydantic models implemented and validated
  - Citation
  - CareerEntry
  - ExecutiveResearchResult
  - DimensionScore
  - MustHaveCheck
  - AssessmentResult
- ‚úÖ Centralized prompt system (demo/prompts/)
  - catalog.yaml: YAML-based prompt definitions for 4 agents
  - library.py: Dynamic prompt loader with get_prompt()
  - Enables code-free prompt editing
- ‚úÖ Context management features
  - add_datetime_to_context=True on research/assessment agents
  - Temporal awareness for "recent" and "current" roles
  - Follows Agno best practices

## Workflow Orchestration

The screening workflow executes a 4-step linear pipeline that coordinates research, quality checking, and assessment agents.

### Execution Flow

```
1. Deep Research ‚Üí 2. Quality Check ‚Üí 3. Incremental Search (conditional) ‚Üí 4. Assessment
```

**Step 1: Deep Research**
- Agent: `o4-mini-deep-research`
- Executes comprehensive candidate research using built-in web search
- Returns markdown report + citations

**Step 2: Quality Gate**
- Heuristic check: `‚â•3 unique citations` AND `non-empty summary`
- Pass ‚Üí proceed to Assessment
- Fail ‚Üí trigger Incremental Search

**Step 3: Incremental Search (Conditional)**
- Agent: `gpt-5-mini` + web_search_preview
- Triggered only when quality gate fails
- Single-pass: max 2 web searches to fill gaps
- Results merged with original research

**Step 4: Assessment**
- Agent: `gpt-5-mini` + ReasoningTools
- Evaluates candidate against role spec
- Outputs structured assessment with dimension scores

### Session State Persistence

**Storage:** SqliteDb at `tmp/agno_sessions.db`

Session state tracks:
- `screen_id`: Airtable record ID
- `candidate_id`: Candidate record ID
- `candidate_name`: For logging
- `last_step`: Most recent completed step
- `quality_gate_triggered`: Whether incremental search ran

**Why SqliteDb?** Persistent audit trail for demo review. InMemoryDb deferred to Phase 2+ fast mode.

### Event Streaming & Logging

**Configuration:** `stream_events=True` on Workflow creation

**Log Format:** INFO level with emoji indicators
- üîç Step start (research, quality check, assessment)
- ‚úÖ Step success with metadata (citation count, scores)
- üîÑ Incremental search triggered
- ‚ùå Errors with context

**Example Output:**
```
2025-11-17 | INFO | üîç Starting deep research for Alex Rivera (CFO at Armis)
2025-11-17 | INFO | ‚úÖ Deep research completed for Alex Rivera with 4 citations
2025-11-17 | INFO | üîç Checking research quality for Alex Rivera
2025-11-17 | INFO | ‚úÖ Research quality threshold met for Alex Rivera
2025-11-17 | INFO | üîç Starting assessment for Alex Rivera
2025-11-17 | INFO | ‚úÖ Assessment complete for Alex Rivera (overall_score=95.0)
```

### Error Handling

**Retry Configuration:** All agents use exponential backoff
- `retries=2` (Deep Research, Assessment)
- `retries=1` (Incremental Search - lightweight)
- `exponential_backoff=True`
- `delay_between_retries=1s`

**Failure Behavior:**
- Agent retries automatically on transient failures
- Session state preserved on errors (no corruption)
- Clear RuntimeError raised after retry exhaustion
- Logs capture failure context with ‚ùå indicator

### Usage Example

```python
from demo.agents import screen_single_candidate

# Candidate data from Airtable People table
candidate = {
    "id": "recABC123",
    "name": "Alex Rivera",
    "current_title": "CFO",
    "current_company": "Armis",
    "linkedin_url": "https://linkedin.com/in/alex-rivera"
}

# Role spec markdown from Airtable Role_Specs table
role_spec = """
# CFO Role Specification
## Must-Haves
- 10+ years finance leadership
- B2B SaaS experience
- Series B+ fundraising
...
"""

# Execute full workflow
assessment = screen_single_candidate(
    candidate_data=candidate,
    role_spec_markdown=role_spec,
    screen_id="recSCREEN001"
)

# Access results
print(f"Overall Score: {assessment.overall_score}")
print(f"Confidence: {assessment.overall_confidence}")
print(f"Summary: {assessment.summary}")

for score in assessment.dimension_scores:
    print(f"{score.dimension}: {score.score}/5 ({score.confidence})")
```

**Output Schema:** Returns `AssessmentResult` Pydantic model with:
- `overall_score`: 0-100 scale (or None if insufficient evidence)
- `overall_confidence`: High/Medium/Low
- `dimension_scores`: List of scored evaluation dimensions
- `summary`: 2-3 sentence topline assessment
- `must_haves_check`: Requirements verification
- `green_flags` / `red_flags_detected`: Signal extraction
- `counterfactuals`: "Why candidate might NOT be ideal"

### Design Decisions

**Linear (not parallel):** Single candidate, synchronous processing. Simplifies v1 implementation and demo presentation. Parallel execution deferred to Phase 2+.

**Quality Gate (not multi-iteration):** Single incremental search pass (max 2 web searches) balances quality improvement with execution speed. Prevents infinite research loops.

**SqliteDb (not InMemoryDb):** Persistent session history enables post-demo review of workflow execution. Critical for demonstrating thinking process to FirstMark team.

**No Custom Event Tables:** Uses Agno-managed session tables only. Custom WorkflowEvent model and event persistence deferred to Phase 2+ production features.

## AgentOS Runtime Setup

The AgentOS FastAPI runtime (`demo/agentos_app.py`) is the canonical `/screen` endpoint for Airtable automations and demo runs. It shares the same workflow logic as the legacy Flask app but adds the AgentOS control plane for live monitoring and configuration.

### Local Development with ngrok

For webhook testing during development, expose your local AgentOS server to the internet using ngrok.

#### Install ngrok

**macOS (Homebrew):**
```bash
brew install ngrok/ngrok/ngrok
```

**Other Platforms:**
Download from [ngrok.com/download](https://ngrok.com/download)

**Verify Installation:**
```bash
ngrok version
```

#### Setup ngrok Authentication

1. Sign up for a free account at [ngrok.com](https://ngrok.com)
2. Get your auth token from the dashboard
3. Configure ngrok:
   ```bash
   ngrok config add-authtoken YOUR_AUTH_TOKEN
   ```

### Running the AgentOS Server

#### 1. Configure Environment Variables

Ensure your `.env` file contains:
```bash
AIRTABLE_API_KEY=patYOUR_PAT_HERE
AIRTABLE_BASE_ID=appYOUR_BASE_ID
OPENAI_API_KEY=sk-YOUR_KEY_HERE
FLASK_HOST=0.0.0.0
FLASK_PORT=5000
FLASK_DEBUG=true
# Optional: enforce bearer auth on AgentOS /screen
# AGENTOS_SECURITY_KEY=super-secret-token
```

#### 2. Start the AgentOS Server

Activate your virtual environment and run the server:
```bash
source .venv/bin/activate
uv run python demo/agentos_app.py
```

You should see:
```
üîç Connecting AgentOS runtime to Airtable base appeY64iIwU5CEna7
INFO:     Started server process [12345]
INFO:     Application startup complete.
```

AgentOS automatically exposes OpenAPI docs at `http://localhost:5000/docs`, runtime metadata at `/config`, and the `/screen` webhook endpoint.

#### 3. Connect to the AgentOS Control Plane

1. Open [https://os.agno.com](https://os.agno.com) and click **Connect OS**
2. Enter your local URL (`http://localhost:5000` or the ngrok URL)
3. Provide the security key if `AGENTOS_SECURITY_KEY` is set (leave blank otherwise)
4. Verify that sessions/runs appear in the dashboard once `/screen` is triggered

#### 4. Start ngrok Tunnel

In a **separate terminal window**, start ngrok:
```bash
ngrok http 5000
```

You'll see output like:
```
Session Status                online
Forwarding                    https://abc123.ngrok.io -> http://localhost:5000
```

**Copy the HTTPS URL** (e.g., `https://abc123.ngrok.io`) - you'll need this for Airtable automation configuration.

### Configuring Airtable Automation

#### 1. Open Your Airtable Base

Navigate to your Talent Signal base (the one with `AIRTABLE_BASE_ID` from your `.env`)

#### 2. Create Automation

1. Click **Automations** in the top toolbar
2. Click **Create automation**
3. Name it: "Trigger Candidate Screening"

#### 3. Configure Trigger

1. **Trigger Type:** When record matches conditions
2. **Table:** Screens
3. **Conditions:**
   - When: `status`
   - Changes to: `Ready to Screen`

#### 4. Add Webhook Action

1. Click **+ Add action**
2. Select **Send a request to a URL**
3. Configure webhook:
   - **Method:** POST
   - **URL:** `https://YOUR_NGROK_URL.ngrok.io/screen` (replace with your ngrok URL)
   - **Headers:**
     - `Content-Type: application/json`
     - `Authorization: Bearer YOUR_AGENTOS_SECURITY_KEY` *(only if configured)*
   - **Body:** JSON
     ```json
     {
       "screen_id": "{RECORD_ID}"
     }
     ```
     (Use the Insert field button to select RECORD_ID)

#### 5. Test the Automation

1. Turn on the automation (toggle in top right)
2. Go to your Screens table
3. Find or create a test Screen record with:
   - Linked candidates (at least one)
   - Linked search with role spec
4. Change the `status` field to "Ready to Screen"

### Manual Testing Workflow

#### Pre-Test Checklist

Before triggering the webhook, verify:

1. ‚úÖ AgentOS server is running (`uv run python demo/agentos_app.py`)
2. ‚úÖ `/docs` responds locally and shows the `/screen` operation
3. ‚úÖ ngrok tunnel is active (check ngrok terminal for "online" status)
4. ‚úÖ Airtable automation is enabled
5. ‚úÖ Test Screen record has:
   - Valid `screen_id` (starts with "rec")
   - At least one linked candidate
   - Linked search with valid role spec
   - Role spec has `structured_spec_markdown` content
6. ‚úÖ Environment variables are set (`.env` loaded)
7. ‚úÖ All dependencies installed (`uv pip list` shows agno, fastapi, pyairtable)

#### Execute Test

1. **Start AgentOS server (if not already running):**
   ```bash
   source .venv/bin/activate
   uv run python demo/agentos_app.py
   ```

2. **Start ngrok (separate terminal):**
   ```bash
   ngrok http 5000
   ```

3. **Configure Airtable automation** with ngrok URL (see above)

4. **Trigger automation:**
   - Open Screens table in Airtable
   - Update test record `status` ‚Üí "Ready to Screen"

5. **Monitor AgentOS logs / control plane:**
   Watch for these log indicators:
   ```
   üîç Received AgentOS screen webhook for recXXXX
   üîç Starting deep research for [Candidate Name]
   ‚úÖ Deep research completed for [Candidate Name] with N citations
   ‚úÖ Assessment complete for [Candidate Name] (overall_score=XX)
   ‚úÖ Screen recXXXX completed (N successes, 0 failures)
   ```

6. **Verify Airtable results:**
   - Screen `status` updated to "Complete"
   - New Assessment records created in Assessments table
   - Assessment records contain:
     - `overall_score` (0-100)
     - `overall_confidence` (High/Medium/Low)
     - `topline_summary` (2-3 sentences)
     - `assessment_json` (full structured output)

#### Test curl Command (Alternative)

If you want to test the endpoint directly without Airtable:
```bash
curl -X POST http://localhost:5000/screen \
  -H "Content-Type: application/json" \
  -d '{"screen_id": "recYOUR_TEST_SCREEN_ID"}'
```

Add the Authorization header if you enabled a security key:
```bash
-H "Authorization: Bearer $AGENTOS_SECURITY_KEY"
```

Expected response:
```json
{
  "status": "success",
  "screen_id": "recXXXX",
  "candidates_total": 1,
  "candidates_processed": 1,
  "candidates_failed": 0,
  "execution_time_seconds": 45.23,
  "results": [
    {
      "candidate_id": "recYYYY",
      "assessment_id": "recZZZZ",
      "overall_score": 85.0,
      "confidence": "High",
      "summary": "Strong CFO candidate with relevant B2B SaaS experience..."
    }
  ]
}
```

### Legacy Flask Server (Optional)

`demo/app.py` remains available for backwards compatibility. Run `python demo/app.py` if you need to reproduce legacy behavior; otherwise prefer `demo/agentos_app.py` for all demos and automation traffic.

### Troubleshooting

#### AgentOS Server Won't Start

**Error:** `Missing required environment variables: AIRTABLE_API_KEY`
- **Fix:** Verify `.env` file exists and contains all required keys
- **Check:** Run `cat .env` to confirm variables are set

**Error:** `Address already in use`
- **Fix:** Kill existing process on port 5000:
  ```bash
  lsof -ti:5000 | xargs kill -9
  ```

#### ngrok Connection Issues

**Error:** `ERR_NGROK_108`
- **Fix:** Verify ngrok auth token is configured:
  ```bash
  ngrok config check
  ```

**Error:** `Session expired`
- **Fix:** Free ngrok sessions expire after 2 hours. Restart ngrok and update Airtable automation URL.

#### Webhook Not Triggering

1. **Check Airtable Automation_Logs:**
   - Open automation
   - Click "Runs" tab
   - Look for errors or failed requests

2. **Verify ngrok URL is correct:**
   - Ensure you're using HTTPS (not HTTP)
   - Include `/screen` path: `https://abc123.ngrok.io/screen`

3. **Test with curl first:**
   - Use the curl command above to verify the AgentOS endpoint works
   - If curl works but Airtable doesn't, issue is with automation config

#### Workflow Execution Errors

**Error:** `Screen missing linked role spec`
- **Fix:** Ensure Screen ‚Üí Search ‚Üí Role Spec linkage is complete

**Error:** `No candidates linked to screen`
- **Fix:** Add at least one candidate to the Screen's "Candidates" field

**Error:** `Role spec missing structured markdown content`
- **Fix:** Populate the `structured_spec_markdown` field in the Role Spec record

### Smoke Test Checklist (Pre-Demo)

Use this checklist to validate the full webhook integration before your demo:

#### Environment Setup
- [ ] Virtual environment activated (`.venv`)
- [ ] All dependencies installed (`uv pip list` confirms fastapi, agno, pyairtable, openai)
- [ ] `.env` file contains all required keys (AIRTABLE_API_KEY, AIRTABLE_BASE_ID, OPENAI_API_KEY)
- [ ] Port 5000 is available (not in use)

#### Airtable Data Preparation
- [ ] Test Screen record exists with status != "Ready to Screen"
- [ ] Screen has ‚â•1 linked candidate with profile data
- [ ] Screen linked to Search record
- [ ] Search linked to Role_Spec record
- [ ] Role_Spec has populated `structured_spec_markdown` field

#### Server & Tunnel
- [ ] AgentOS server starts without errors (`uv run python demo/agentos_app.py`)
- [ ] `/docs` responds locally and shows `/screen`
- [ ] ngrok tunnel active (`ngrok http 5000`)
- [ ] ngrok shows "Session Status: online"
- [ ] Copied ngrok HTTPS URL (e.g., `https://abc123.ngrok.io`)

#### Airtable Automation Configuration
- [ ] Automation created in Airtable
- [ ] Trigger: "When record matches conditions" ‚Üí Screens ‚Üí status = "Ready to Screen"
- [ ] Action: "Send request" ‚Üí POST ‚Üí `https://YOUR_NGROK_URL/screen`
- [ ] Request body includes `{"screen_id": "{RECORD_ID}"}`
- [ ] Automation is enabled (toggle ON)

#### Execution Test
- [ ] Changed Screen status ‚Üí "Ready to Screen" in Airtable
- [ ] AgentOS logs show: "üîç Received AgentOS screen webhook for recXXXX"
- [ ] Workflow executes without ‚ùå errors
- [ ] AgentOS logs show: "‚úÖ Screen recXXXX completed"
- [ ] Screen status updated to "Complete" in Airtable
- [ ] New Assessment record(s) created in Assessments table
- [ ] Assessment contains:
  - [ ] `overall_score` (numeric, 0-100)
  - [ ] `overall_confidence` (High/Medium/Low)
  - [ ] `topline_summary` (non-empty text)
  - [ ] `assessment_json` (valid JSON)

#### Post-Test Verification
- [ ] Execution time <10 minutes per candidate
- [ ] No memory errors or crashes
- [ ] Logs are readable and helpful (emoji indicators visible)
- [ ] Can trigger multiple Screens sequentially without server restart

#### Demo Readiness
- [ ] 3+ pre-run Screens completed successfully (Pigment CFO, Mockingbird CFO, Synthesia CTO)
- [ ] 1 live demo Screen prepared (Estuary CTO) with status = "Pending" (not triggered yet)
- [ ] AgentOS control plane connected (optional)
- [ ] ngrok tunnel URL documented for live demo
- [ ] Backup plan if ngrok fails (curl command ready)

**Note:** For production deployment, replace ngrok with a proper hosting solution (Cloud Run, Heroku, etc.) with static URLs.

## Case Study Resources

- **Directory:** `case/`
- **Brief:** `case/FirstMark_case.md`
- **Specifications:** `spec/`
- **Reference Materials:** `reference/`
